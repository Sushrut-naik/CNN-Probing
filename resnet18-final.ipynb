{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8354058,"sourceType":"datasetVersion","datasetId":4963783}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#imports\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport pandas as pd\nimport torchvision\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation, ColorJitter, RandomResizedCrop, RandomApply, RandomAffine\nfrom os.path import join\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.527154Z","iopub.execute_input":"2024-05-08T09:18:05.527449Z","iopub.status.idle":"2024-05-08T09:18:05.538380Z","shell.execute_reply.started":"2024-05-08T09:18:05.527406Z","shell.execute_reply":"2024-05-08T09:18:05.537556Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### define global variables","metadata":{}},{"cell_type":"code","source":"images_path = '/kaggle/input/trifeature/trifeature-dataset/color_texture_shape_stimuli/color_texture_shape_stimuli'","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.539380Z","iopub.execute_input":"2024-05-08T09:18:05.539665Z","iopub.status.idle":"2024-05-08T09:18:05.548893Z","shell.execute_reply.started":"2024-05-08T09:18:05.539642Z","shell.execute_reply":"2024-05-08T09:18:05.548194Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"version_network = 'version_0'\nversion_decoder = 'version_1'","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.550640Z","iopub.execute_input":"2024-05-08T09:18:05.550900Z","iopub.status.idle":"2024-05-08T09:18:05.558243Z","shell.execute_reply.started":"2024-05-08T09:18:05.550873Z","shell.execute_reply":"2024-05-08T09:18:05.557559Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"layer_names = ['layer4','avgpool', 'fc']","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.559271Z","iopub.execute_input":"2024-05-08T09:18:05.559603Z","iopub.status.idle":"2024-05-08T09:18:05.567580Z","shell.execute_reply.started":"2024-05-08T09:18:05.559572Z","shell.execute_reply":"2024-05-08T09:18:05.566723Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"markdown","source":"#### get path of labels mapping and log files","metadata":{}},{"cell_type":"code","source":"def get_labels_logs_path(feature_name, version):\n    if(feature_name == 'color'): \n        return '/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/color_splits/splits.pkl', f'/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/color_splits/logs/{version}_split.txt'\n    elif(feature_name == 'shape'): \n        return '/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/shape_splits/splits.pkl', f'/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/shape_splits/logs/{version}_split.txt'\n    elif(feature_name == 'texture'):\n        return '/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/texture_splits/splits.pkl', f'/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/texture_splits/logs/{version}_split.txt'\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.568661Z","iopub.execute_input":"2024-05-08T09:18:05.570624Z","iopub.status.idle":"2024-05-08T09:18:05.579047Z","shell.execute_reply.started":"2024-05-08T09:18:05.570598Z","shell.execute_reply":"2024-05-08T09:18:05.578207Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Extract truth labels as list for decoder y_test, y_train","metadata":{}},{"cell_type":"code","source":"def get_Truthlabels_list(feature_name, version, train_val):\n    decoder_labels_path, log_path = get_labels_logs_path(feature_name, version)\n    loaded_object = get_input_labels(decoder_labels_path)\n    loaded_object = loaded_object[version][train_val]\n    truth_labels = [item[feature_name] for item in loaded_object]\n    class_dict = get_class_labels_dict(log_path, feature_name)\n    labels_numeric = [class_dict[label] for label in truth_labels]\n    return labels_numeric","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.641589Z","iopub.execute_input":"2024-05-08T09:18:05.642304Z","iopub.status.idle":"2024-05-08T09:18:05.648059Z","shell.execute_reply.started":"2024-05-08T09:18:05.642279Z","shell.execute_reply":"2024-05-08T09:18:05.647002Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### extract labels as dictionary from pickle file","metadata":{}},{"cell_type":"code","source":"import pickle\n\ndef get_input_labels(path):\n    # Open the .pkl file for reading in binary mode\n    with open(path, 'rb') as f:\n        # Load the object from the file\n        loaded_object = pickle.load(f)\n#         list of dictionaries\n        return loaded_object","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.673774Z","iopub.execute_input":"2024-05-08T09:18:05.674096Z","iopub.status.idle":"2024-05-08T09:18:05.686467Z","shell.execute_reply.started":"2024-05-08T09:18:05.674066Z","shell.execute_reply":"2024-05-08T09:18:05.685592Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_classList(feature_name):\n    color_class = [\"red\", \"green\", \"blue\", \"yellow\", \"pink\", \"cyan\", \"purple\", \"ocean\", \"orange\", \"white\"]\n    shape_class = [\"triangle\", \"square\", \"plus\", \"circle\", \"tee\", \"rhombus\", \"pentagon\", \"star\", \"fivesquare\", \"trapezoid\"]\n    texture_class = [\"solid\", \"stripes\", \"grid\", \"hexgrid\", \"dots\", \"noise\", \"triangles\", \"zigzags\", \"rain\", \"pluses\"]\n    \n    if(feature_name == 'color'):\n        return color_class\n    elif(feature_name == 'shape'):\n        return shape_class\n    elif(feature_name == 'texture'):\n        return texture_class","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.688741Z","iopub.execute_input":"2024-05-08T09:18:05.689017Z","iopub.status.idle":"2024-05-08T09:18:05.696815Z","shell.execute_reply.started":"2024-05-08T09:18:05.688994Z","shell.execute_reply":"2024-05-08T09:18:05.696102Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_class_labels_dict(txt_file, feature_name):\n    with open(txt_file, 'r') as file:\n        content = file.readlines()\n\n    # Iterate through the lines to find the shape classes\n    classes = None\n    for line in content:\n        if feature_name in line:\n            # Extract the shape classes\n            classes = line.split(':')[1].strip()[1:-1].split(', ')\n            classes = [cls.strip().strip(\"'\") for cls in classes]\n#             break\n\n    # Print the list of shape classes\n    print(f\"List of {feature_name} hold out classes:\", classes)\n    \n    original_class_list = get_classList(feature_name)\n    # Remove shape classes from the original list\n    remaining_items = [item for item in original_class_list if item not in classes]\n\n    # Create a 0-indexed dictionary of remaining items\n    indexed_dict = {item: index for index, item in enumerate(remaining_items)}\n\n    # Print the indexed dictionary\n    print(\"0-indexed dictionary of remaining items:\", indexed_dict)\n    \n    return indexed_dict\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.697845Z","iopub.execute_input":"2024-05-08T09:18:05.698090Z","iopub.status.idle":"2024-05-08T09:18:05.708299Z","shell.execute_reply.started":"2024-05-08T09:18:05.698069Z","shell.execute_reply":"2024-05-08T09:18:05.707621Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train_val_labels_path - pickle file\ndef getAllDataLoders(feature_name, b_size=64, version = 'version_0', shuffle_train=True):\n    \n    train_val_labels_path, log_path = get_labels_logs_path(feature_name, version)\n    # get list of label mappings\n    \n    train_val_labels = get_input_labels(train_val_labels_path)[version]\n    train_labels = train_val_labels['train']\n    val_labels = train_val_labels['val']\n    \n    # create dataset\n    train_dataset = getDataset(train_labels, log_path, feature_name  )\n    val_dataset = getDataset(val_labels, log_path, feature_name )\n\n    train_loader = DataLoader(train_dataset, batch_size=b_size, shuffle = shuffle_train)\n    val_loader = DataLoader(val_dataset, batch_size=b_size, shuffle=False)\n\n    \n    return train_loader, val_loader\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.709345Z","iopub.execute_input":"2024-05-08T09:18:05.709638Z","iopub.status.idle":"2024-05-08T09:18:05.722083Z","shell.execute_reply.started":"2024-05-08T09:18:05.709615Z","shell.execute_reply":"2024-05-08T09:18:05.721313Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def getDataset(labels, txt_file, feature_name):\n    \n    classes_dict = get_class_labels_dict(txt_file, feature_name)\n    dataset = TrifeatureDataset(images_path, labels, classes_dict, feature_name)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.723032Z","iopub.execute_input":"2024-05-08T09:18:05.724097Z","iopub.status.idle":"2024-05-08T09:18:05.732686Z","shell.execute_reply.started":"2024-05-08T09:18:05.724072Z","shell.execute_reply":"2024-05-08T09:18:05.731913Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Define Model","metadata":{}},{"cell_type":"code","source":"# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.733791Z","iopub.execute_input":"2024-05-08T09:18:05.734343Z","iopub.status.idle":"2024-05-08T09:18:05.764521Z","shell.execute_reply.started":"2024-05-08T09:18:05.734312Z","shell.execute_reply":"2024-05-08T09:18:05.763539Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def getModel(num_classes=7):\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(\"Device:\", device)\n\n    # Set random seed for reproducibility\n    torch.manual_seed(42)\n\n    # Load ResNet50 model\n    model = models.resnet18(pretrained=False)\n    \n    # Replace the fully connected layer with a new one for the desired number of classes\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, num_classes)\n\n    model = model.to(device)\n\n    return model\n\n    # model.eval()\n    # Print model summary\n    # print(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.784292Z","iopub.execute_input":"2024-05-08T09:18:05.784581Z","iopub.status.idle":"2024-05-08T09:18:05.794695Z","shell.execute_reply.started":"2024-05-08T09:18:05.784558Z","shell.execute_reply":"2024-05-08T09:18:05.793991Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = getModel()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:05.795768Z","iopub.execute_input":"2024-05-08T09:18:05.796061Z","iopub.status.idle":"2024-05-08T09:18:06.227576Z","shell.execute_reply.started":"2024-05-08T09:18:05.796038Z","shell.execute_reply":"2024-05-08T09:18:06.226692Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=7, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Make Dataset\n* requires classes_dict of 7 labels of target feature","metadata":{}},{"cell_type":"code","source":"class TrifeatureDataset(torch.utils.data.Dataset):\n#  details list - maps image to its actual labels\n#  classes_dict - dictionary of 7 classes of the target feature\n# ex - 'red':0, 'blue':1  this helps to map colour to a index for final output layer\n    def __init__(self, img_path, details_list, classes_dict, feature):\n        super(TrifeatureDataset, self).__init__()\n\n        self.img_path = img_path\n        self.details_list = details_list\n        self.feature = feature\n\n        self.transform = self._transform(224)\n        \n        self.classes = classes_dict\n\n\n    @staticmethod    \n    def _convert_image_to_rgb(image):\n        return image.convert(\"RGB\")\n\n    def _transform(self, n_px):\n        mean = [0.50190921, 0.50194219, 0.49818846]\n        std =  [0.1426835,  0.1282568,  0.13595397]\n        return Compose([\n            Resize(n_px),\n            self._convert_image_to_rgb,\n            ToTensor(),\n            Normalize(mean, std)\n        ])\n\n    def read_img(self, file_name):\n        im_path = join(self.img_path,file_name)   \n        img = Image.open(im_path)\n        img = self.transform(img)\n        return img\n\n    def __getitem__(self, index):\n        file_name = self.details_list[index]['fname']\n        img = self.read_img(file_name)\n        target_label = self.details_list[index][self.feature]\n        return img, self.classes[target_label]\n\n\n    def __len__(self):\n        return len(self.details_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.228592Z","iopub.execute_input":"2024-05-08T09:18:06.228842Z","iopub.status.idle":"2024-05-08T09:18:06.240513Z","shell.execute_reply.started":"2024-05-08T09:18:06.228821Z","shell.execute_reply":"2024-05-08T09:18:06.239388Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### make dataset","metadata":{}},{"cell_type":"code","source":"def getDataLoader(dataset, batch_size=64, shuffle = True):\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.244822Z","iopub.execute_input":"2024-05-08T09:18:06.245143Z","iopub.status.idle":"2024-05-08T09:18:06.254291Z","shell.execute_reply.started":"2024-05-08T09:18:06.245115Z","shell.execute_reply":"2024-05-08T09:18:06.253547Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"best_val_path = 'best_val.pth'\nbest_val_loss = float('inf')\nbest_train_loss = float('inf')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.267287Z","iopub.execute_input":"2024-05-08T09:18:06.267660Z","iopub.status.idle":"2024-05-08T09:18:06.275919Z","shell.execute_reply.started":"2024-05-08T09:18:06.267610Z","shell.execute_reply":"2024-05-08T09:18:06.274879Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, criterion, best_val_loss=float('inf'), num_epochs=30):\n    print(best_val_loss)\n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item() * images.size(0)\n\n        avg_train_loss = total_train_loss / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        total_val_loss = 0\n        correct_predictions = 0\n        total_predictions = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                total_val_loss += loss.item() * images.size(0)\n                \n                _, predicted = torch.max(outputs.data, 1)\n                total_predictions += labels.size(0)\n                correct_predictions += (predicted == labels).sum().item()\n\n        avg_val_loss = total_val_loss / len(val_loader.dataset)\n        val_accuracy = correct_predictions / total_predictions\n        print(\"val accuracy:\", val_accuracy)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n        \n        \n        # Save the model if validation loss has decreased\n        if avg_val_loss < best_val_loss:\n            print('Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...'.format(\n                best_val_loss,\n                avg_val_loss))\n            torch.save(model.state_dict(), best_val_path)\n            best_val_loss = avg_val_loss\n        print(f\"Epoch number{epoch+1}: Train Loss:{avg_train_loss}, Val Loss:{avg_val_loss}\")\n    return best_val_loss\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.278483Z","iopub.execute_input":"2024-05-08T09:18:06.279690Z","iopub.status.idle":"2024-05-08T09:18:06.293323Z","shell.execute_reply.started":"2024-05-08T09:18:06.279654Z","shell.execute_reply":"2024-05-08T09:18:06.292204Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"### Get Embeddings from intermediate layers","metadata":{}},{"cell_type":"code","source":"# Hook function to store activations\ndef hook_fn(module, input, output, name, activations):\n    if name not in activations:\n        activations[name] = []\n    activations[name].append(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.313032Z","iopub.execute_input":"2024-05-08T09:18:06.313344Z","iopub.status.idle":"2024-05-08T09:18:06.321940Z","shell.execute_reply.started":"2024-05-08T09:18:06.313316Z","shell.execute_reply":"2024-05-08T09:18:06.320844Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\ndef getEmbeddings(model, input_dataloader , layer_names = ['avgpool', 'fc']):\n    # Dictionary to store the activations of selected layers\n    activations = {}\n    hook_handles = {}\n\n    # Register hooks on the selected layers\n    for name, module in model.named_modules():\n        if name in layer_names:\n            handle = module.register_forward_hook(lambda m, i, o, name=name: hook_fn(m, i, o, name, activations))\n            hook_handles[name] = handle\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Forward pass\n    with torch.no_grad():\n        for images, labels in input_dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            \n    for key, val in hook_handles.items():\n        hook_handles[key].remove()\n\n    # Extract and print activations\n    for name, activation in activations.items():\n        print(f'Activation of layer {name}: Shape={len(activation)}')\n        \n    return activations","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.323294Z","iopub.execute_input":"2024-05-08T09:18:06.324686Z","iopub.status.idle":"2024-05-08T09:18:06.336452Z","shell.execute_reply.started":"2024-05-08T09:18:06.324652Z","shell.execute_reply":"2024-05-08T09:18:06.335491Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model = getModel()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.337647Z","iopub.execute_input":"2024-05-08T09:18:06.337941Z","iopub.status.idle":"2024-05-08T09:18:06.580184Z","shell.execute_reply.started":"2024-05-08T09:18:06.337914Z","shell.execute_reply":"2024-05-08T09:18:06.579195Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.avgpool)\nprint(model.fc)\nprint(model.named_modules)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.581416Z","iopub.execute_input":"2024-05-08T09:18:06.581720Z","iopub.status.idle":"2024-05-08T09:18:06.587503Z","shell.execute_reply.started":"2024-05-08T09:18:06.581695Z","shell.execute_reply":"2024-05-08T09:18:06.586545Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"AdaptiveAvgPool2d(output_size=(1, 1))\nLinear(in_features=512, out_features=7, bias=True)\n<bound method Module.named_modules of ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=7, bias=True)\n)>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.588821Z","iopub.execute_input":"2024-05-08T09:18:06.589477Z","iopub.status.idle":"2024-05-08T09:18:06.598412Z","shell.execute_reply.started":"2024-05-08T09:18:06.589428Z","shell.execute_reply":"2024-05-08T09:18:06.597494Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=7, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model.layer4[1].conv2)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.599775Z","iopub.execute_input":"2024-05-08T09:18:06.600084Z","iopub.status.idle":"2024-05-08T09:18:06.616262Z","shell.execute_reply.started":"2024-05-08T09:18:06.600058Z","shell.execute_reply":"2024-05-08T09:18:06.615312Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Defining a neural network for Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## Train Decoder","metadata":{}},{"cell_type":"code","source":"class LogisticRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.708617Z","iopub.execute_input":"2024-05-08T09:18:06.708932Z","iopub.status.idle":"2024-05-08T09:18:06.716674Z","shell.execute_reply.started":"2024-05-08T09:18:06.708905Z","shell.execute_reply":"2024-05-08T09:18:06.715866Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class DecoderDataset(Dataset):\n    def __init__(self, inputs, labels):\n        self.inputs = inputs\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, idx):\n        input_data = torch.tensor(self.inputs[idx], dtype=torch.float)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return input_data, label\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.728247Z","iopub.execute_input":"2024-05-08T09:18:06.728599Z","iopub.status.idle":"2024-05-08T09:18:06.737833Z","shell.execute_reply.started":"2024-05-08T09:18:06.728568Z","shell.execute_reply":"2024-05-08T09:18:06.736916Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def decoder_accuracy(model, dataloader=None, embeddings=None, labels=None):\n    \n    if(not dataloader):\n        np_list = [np.array(tensor.cpu()) for tensor in embeddings]\n        embeddings_np = np.array(np_list)\n        embeddings_np = embeddings_np.reshape(embeddings_np.shape[0], -1)\n        input_dim = embeddings_np.shape[1]\n        output_dim = 7\n        dataset = DecoderDataset(embeddings_np, labels)\n\n        dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n    \n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs.float())\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.738866Z","iopub.execute_input":"2024-05-08T09:18:06.739178Z","iopub.status.idle":"2024-05-08T09:18:06.752422Z","shell.execute_reply.started":"2024-05-08T09:18:06.739149Z","shell.execute_reply":"2024-05-08T09:18:06.751593Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"best_decoder_path = 'best_decoder.pth'","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.764542Z","iopub.execute_input":"2024-05-08T09:18:06.764850Z","iopub.status.idle":"2024-05-08T09:18:06.778882Z","shell.execute_reply.started":"2024-05-08T09:18:06.764823Z","shell.execute_reply":"2024-05-08T09:18:06.778013Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def train_custom_decoder(train_embeddings, train_labels, lr=0.001, num_epochs=200):\n    \n    np_list = [np.array(tensor.cpu()) for tensor in train_embeddings]\n    train_embeddings_np = np.array(np_list)\n    train_embeddings_np = train_embeddings_np.reshape(train_embeddings_np.shape[0], -1)\n    input_dim = train_embeddings_np.shape[1]\n    output_dim = 7\n    dataset = DecoderDataset(train_embeddings_np, train_labels)\n    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n    \n    model = LogisticRegression(input_dim, output_dim)\n    model = model.to(device)\n    \n    best_loss = float('inf')\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr= lr)\n\n    count=0\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs.float())\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * inputs.size(0)\n        \n        epoch_loss /= len(train_loader.dataset)  # Compute average epoch loss\n        if epoch_loss < best_loss:\n#             print(\"saving model\")\n            count+=1\n            best_loss = epoch_loss\n            torch.save(model.state_dict(), best_decoder_path)\n        \n        if (epoch+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, saved_times: {count}')\n            count=0\n    print(f\"Best model saved with loss: {best_loss:.4f}\")\n    model.load_state_dict(torch.load(best_decoder_path))\n    return model, decoder_accuracy(model, dataloader = train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:26:52.748526Z","iopub.execute_input":"2024-05-08T09:26:52.749350Z","iopub.status.idle":"2024-05-08T09:26:52.760176Z","shell.execute_reply.started":"2024-05-08T09:26:52.749320Z","shell.execute_reply":"2024-05-08T09:26:52.759186Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"### Decoder Test accuracy","metadata":{}},{"cell_type":"markdown","source":"# whole pipeline:\n* train original network on:\n    shape, color, texture\n  also take untrained network\n* for each of the above networks train and test decoder independently on the features:\n    shape, color, texture","metadata":{}},{"cell_type":"code","source":"# learning_rate = {'avgpool': 0.001, 'fc': 0.000001}\nlearning_rate = {'layer4':0.01 , 'avgpool': 0.01, 'fc': 0.01}\n\n# layer_names = ['avgpool', 'fc']","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.820740Z","iopub.execute_input":"2024-05-08T09:18:06.821065Z","iopub.status.idle":"2024-05-08T09:18:06.829460Z","shell.execute_reply.started":"2024-05-08T09:18:06.821036Z","shell.execute_reply":"2024-05-08T09:18:06.828600Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def whole_pipeline(target_feature, train_embeddings=None, test_embeddings=None):\n\n    model = getModel()\n    # Define the loss criterion\n    criterion = nn.CrossEntropyLoss()\n    # Define the optimizer\n    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n    if(target_feature != 'untrained'):\n        print(\"training network model for target feature:\", target_feature)\n        train_val_labels_path = get_labels_logs_path(target_feature, version_network)\n        train_loader, val_loader = getAllDataLoders(target_feature) \n#         if(target_feature != 'shape'):\n        train_model(model, train_loader, val_loader, optimizer, criterion, best_val_loss, num_epochs=12)\n        model.load_state_dict(torch.load('best_val.pth'))\n        \n        \n    # find embeddings for version 1\n    features = ['shape', 'color', 'texture']\n    accuracies = {}\n    \n    for feature_name in features:\n        print(\"decoding feature:\", feature_name)\n        decoder_train_loader, decoder_val_loader = getAllDataLoders( feature_name, 1, version = version_decoder, shuffle_train=False)       \n        train_embeddings = getEmbeddings(model, decoder_train_loader , layer_names = layer_names)\n        test_embeddings = getEmbeddings(model, decoder_val_loader , layer_names = layer_names)\n\n\n        train_labels = get_Truthlabels_list(feature_name, version_decoder, 'train')        \n        \n        \n        test_labels = get_Truthlabels_list(feature_name, version_decoder, 'val')\n\n        layer_accuracies = {}\n        for layer in layer_names:\n            print(f\"learning rate for {layer} =\", learning_rate[layer])\n            \n            decoder, decoder_acc_train = train_custom_decoder(train_embeddings[layer] , train_labels, learning_rate[layer], num_epochs=300)\n            \n            print(f\"decoder train Accuracy for layer {layer}: {decoder_acc_train:.2f}\")\n            # Predict on the test set\n            decoder_acc_test = decoder_accuracy(decoder, embeddings = test_embeddings[layer], labels = test_labels)\n            print(f\"test Accuracy for layer {layer}: {decoder_acc_test:.2f}\")\n            layer_accuracies[layer] = (decoder_acc_train, decoder_acc_test)\n        accuracies[feature_name] = layer_accuracies\n\n    return accuracies, train_embeddings\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:59:10.415657Z","iopub.execute_input":"2024-05-08T10:59:10.416525Z","iopub.status.idle":"2024-05-08T10:59:10.428110Z","shell.execute_reply.started":"2024-05-08T10:59:10.416484Z","shell.execute_reply":"2024-05-08T10:59:10.427207Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"embeddings_path = 'embeddings_file.pkl'\ndecoding_acc_path = 'decoding_acc.pkl'","metadata":{"execution":{"iopub.status.busy":"2024-05-08T09:18:06.844636Z","iopub.execute_input":"2024-05-08T09:18:06.845379Z","iopub.status.idle":"2024-05-08T09:18:06.855949Z","shell.execute_reply.started":"2024-05-08T09:18:06.845347Z","shell.execute_reply":"2024-05-08T09:18:06.855065Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"def main():\n    target_features = [ 'shape', 'untrained', 'color', 'texture']\n    \n    model_accuracies = {} \n    embeddings = {}\n#     model_accuracies = {'target_feature': {'feature_name': {'layer_name': accuracy_value}}}\n    for target_feature in target_features:\n        # get accuracy of current model for all features\n        print(\"network model train target feature:\", target_feature)\n        accuracy_dict, emb = whole_pipeline(target_feature)\n\n        model_accuracies[target_feature] = accuracy_dict\n        embeddings[target_feature] = emb\n        \n    with open(embeddings_path, 'wb') as f:\n        pickle.dump(embeddings, f)\n    \n    with open(decoding_acc_path, 'wb') as f:\n        pickle.dump(model_accuracies, f)\n    print(\"decoding accuracies:\", model_accuracies)\n    return model_accuracies\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:58:44.811377Z","iopub.execute_input":"2024-05-08T10:58:44.811767Z","iopub.status.idle":"2024-05-08T10:58:44.818951Z","shell.execute_reply.started":"2024-05-08T10:58:44.811737Z","shell.execute_reply":"2024-05-08T10:58:44.817856Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"model_acc = main()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:59:19.641735Z","iopub.execute_input":"2024-05-08T10:59:19.642458Z","iopub.status.idle":"2024-05-08T11:55:42.956135Z","shell.execute_reply.started":"2024-05-08T10:59:19.642407Z","shell.execute_reply":"2024-05-08T11:55:42.955180Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"network model train target feature: shape\nDevice: cuda\ntraining network model for target feature: shape\nList of shape hold out classes: ['trapezoid', 'pentagon', 'square']\n0-indexed dictionary of remaining items: {'triangle': 0, 'plus': 1, 'circle': 2, 'tee': 3, 'rhombus': 4, 'star': 5, 'fivesquare': 6}\nList of shape hold out classes: ['trapezoid', 'pentagon', 'square']\n0-indexed dictionary of remaining items: {'triangle': 0, 'plus': 1, 'circle': 2, 'tee': 3, 'rhombus': 4, 'star': 5, 'fivesquare': 6}\ninf\nval accuracy: 0.5397759103641456\nEpoch 1/12, Train Loss: 1.0577, Val Loss: 1.6940\nValidation loss decreased (inf --> 1.6940).  Saving model ...\nEpoch number1: Train Loss:1.0576514619953779, Val Loss:1.693976469534118\nval accuracy: 0.8518207282913165\nEpoch 2/12, Train Loss: 0.2587, Val Loss: 0.4292\nValidation loss decreased (1.6940 --> 0.4292).  Saving model ...\nEpoch number2: Train Loss:0.2587496824671159, Val Loss:0.42923013711509916\nval accuracy: 0.9635854341736695\nEpoch 3/12, Train Loss: 0.0616, Val Loss: 0.1360\nValidation loss decreased (0.4292 --> 0.1360).  Saving model ...\nEpoch number3: Train Loss:0.06160069715133909, Val Loss:0.13599067847935759\nval accuracy: 0.9658263305322129\nEpoch 4/12, Train Loss: 0.0132, Val Loss: 0.1220\nValidation loss decreased (0.1360 --> 0.1220).  Saving model ...\nEpoch number4: Train Loss:0.013233050669247485, Val Loss:0.1220377535796633\nval accuracy: 0.969187675070028\nEpoch 5/12, Train Loss: 0.0070, Val Loss: 0.1094\nValidation loss decreased (0.1220 --> 0.1094).  Saving model ...\nEpoch number5: Train Loss:0.007032127824473485, Val Loss:0.10935718247703477\nval accuracy: 0.9717086834733893\nEpoch 6/12, Train Loss: 0.0047, Val Loss: 0.0805\nValidation loss decreased (0.1094 --> 0.0805).  Saving model ...\nEpoch number6: Train Loss:0.004748613706656865, Val Loss:0.08051716240764666\nval accuracy: 0.9719887955182073\nEpoch 7/12, Train Loss: 0.0030, Val Loss: 0.1099\nEpoch number7: Train Loss:0.003041507437032357, Val Loss:0.10986780969201684\nval accuracy: 0.9717086834733893\nEpoch 8/12, Train Loss: 0.0026, Val Loss: 0.0845\nEpoch number8: Train Loss:0.002586701269188228, Val Loss:0.08445756082691733\nval accuracy: 0.9714285714285714\nEpoch 9/12, Train Loss: 0.0020, Val Loss: 0.0867\nEpoch number9: Train Loss:0.001990823550747148, Val Loss:0.08668505192971697\nval accuracy: 0.9775910364145658\nEpoch 10/12, Train Loss: 0.0018, Val Loss: 0.0777\nValidation loss decreased (0.0805 --> 0.0777).  Saving model ...\nEpoch number10: Train Loss:0.00184016981428223, Val Loss:0.07770568263672648\nval accuracy: 0.9775910364145658\nEpoch 11/12, Train Loss: 0.0016, Val Loss: 0.0772\nValidation loss decreased (0.0777 --> 0.0772).  Saving model ...\nEpoch number11: Train Loss:0.0015726548720031716, Val Loss:0.07721810150547188\nval accuracy: 0.9787114845938375\nEpoch 12/12, Train Loss: 0.0017, Val Loss: 0.0754\nValidation loss decreased (0.0772 --> 0.0754).  Saving model ...\nEpoch number12: Train Loss:0.0017269043788129231, Val Loss:0.07540852310610753\ndecoding feature: shape\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 47\nEpoch [200/300], Loss: 0.0000, saved_times: 40\nEpoch [300/300], Loss: 0.0000, saved_times: 4\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 1.00\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.0002, saved_times: 31\nEpoch [200/300], Loss: 0.0000, saved_times: 22\nEpoch [300/300], Loss: 0.0000, saved_times: 23\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer avgpool: 1.00\ntest Accuracy for layer avgpool: 1.00\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 0.0648, saved_times: 68\nEpoch [200/300], Loss: 0.0574, saved_times: 16\nEpoch [300/300], Loss: 0.0530, saved_times: 16\nBest model saved with loss: 0.0525\ndecoder train Accuracy for layer fc: 0.99\ntest Accuracy for layer fc: 0.99\ndecoding feature: color\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 1.4495, saved_times: 15\nEpoch [200/300], Loss: 0.0000, saved_times: 2\nEpoch [300/300], Loss: 0.0000, saved_times: 0\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.65\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.0987, saved_times: 46\nEpoch [200/300], Loss: 0.0092, saved_times: 19\nEpoch [300/300], Loss: 0.0043, saved_times: 8\nBest model saved with loss: 0.0018\ndecoder train Accuracy for layer avgpool: 1.00\ntest Accuracy for layer avgpool: 0.80\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 1.7211, saved_times: 14\nEpoch [200/300], Loss: 1.7110, saved_times: 2\nEpoch [300/300], Loss: 1.7123, saved_times: 3\nBest model saved with loss: 1.7049\ndecoder train Accuracy for layer fc: 0.34\ntest Accuracy for layer fc: 0.33\ndecoding feature: texture\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 3.0898, saved_times: 22\nEpoch [200/300], Loss: 0.2241, saved_times: 3\nEpoch [300/300], Loss: 0.2082, saved_times: 2\nBest model saved with loss: 0.0571\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.25\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 1.4382, saved_times: 29\nEpoch [200/300], Loss: 1.3346, saved_times: 10\nEpoch [300/300], Loss: 1.2713, saved_times: 7\nBest model saved with loss: 1.1406\ndecoder train Accuracy for layer avgpool: 0.59\ntest Accuracy for layer avgpool: 0.31\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 1.9610, saved_times: 10\nEpoch [200/300], Loss: 1.9615, saved_times: 3\nEpoch [300/300], Loss: 1.9531, saved_times: 2\nBest model saved with loss: 1.9463\ndecoder train Accuracy for layer fc: 0.16\ntest Accuracy for layer fc: 0.15\nnetwork model train target feature: untrained\nDevice: cuda\ndecoding feature: shape\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 86\nEpoch [200/300], Loss: 0.0000, saved_times: 100\nEpoch [300/300], Loss: 0.0000, saved_times: 82\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.95\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.1369, saved_times: 44\nEpoch [200/300], Loss: 0.0591, saved_times: 21\nEpoch [300/300], Loss: 0.1254, saved_times: 11\nBest model saved with loss: 0.0399\ndecoder train Accuracy for layer avgpool: 0.99\ntest Accuracy for layer avgpool: 0.94\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 1.4883, saved_times: 88\nEpoch [200/300], Loss: 1.4516, saved_times: 44\nEpoch [300/300], Loss: 1.4389, saved_times: 24\nBest model saved with loss: 1.4383\ndecoder train Accuracy for layer fc: 0.42\ntest Accuracy for layer fc: 0.41\ndecoding feature: color\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 62\nEpoch [200/300], Loss: 0.0000, saved_times: 86\nEpoch [300/300], Loss: 0.0000, saved_times: 22\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.98\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.0042, saved_times: 74\nEpoch [200/300], Loss: 0.0007, saved_times: 38\nEpoch [300/300], Loss: 0.0003, saved_times: 41\nBest model saved with loss: 0.0003\ndecoder train Accuracy for layer avgpool: 1.00\ntest Accuracy for layer avgpool: 1.00\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 0.6130, saved_times: 100\nEpoch [200/300], Loss: 0.5073, saved_times: 93\nEpoch [300/300], Loss: 0.4699, saved_times: 63\nBest model saved with loss: 0.4699\ndecoder train Accuracy for layer fc: 0.83\ntest Accuracy for layer fc: 0.80\ndecoding feature: texture\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 67\nEpoch [200/300], Loss: 0.0000, saved_times: 100\nEpoch [300/300], Loss: 0.0000, saved_times: 100\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.34\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 1.2840, saved_times: 30\nEpoch [200/300], Loss: 1.0641, saved_times: 9\nEpoch [300/300], Loss: 1.0318, saved_times: 7\nBest model saved with loss: 0.9136\ndecoder train Accuracy for layer avgpool: 0.66\ntest Accuracy for layer avgpool: 0.38\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 1.8608, saved_times: 47\nEpoch [200/300], Loss: 1.8580, saved_times: 11\nEpoch [300/300], Loss: 1.8577, saved_times: 3\nBest model saved with loss: 1.8555\ndecoder train Accuracy for layer fc: 0.22\ntest Accuracy for layer fc: 0.19\nnetwork model train target feature: color\nDevice: cuda\ntraining network model for target feature: color\nList of color hold out classes: ['orange', 'blue', 'purple']\n0-indexed dictionary of remaining items: {'red': 0, 'green': 1, 'yellow': 2, 'pink': 3, 'cyan': 4, 'ocean': 5, 'white': 6}\nList of color hold out classes: ['orange', 'blue', 'purple']\n0-indexed dictionary of remaining items: {'red': 0, 'green': 1, 'yellow': 2, 'pink': 3, 'cyan': 4, 'ocean': 5, 'white': 6}\ninf\nval accuracy: 1.0\nEpoch 1/12, Train Loss: 0.2043, Val Loss: 0.0030\nValidation loss decreased (inf --> 0.0030).  Saving model ...\nEpoch number1: Train Loss:0.20427014169842547, Val Loss:0.0030419575870271865\nval accuracy: 1.0\nEpoch 2/12, Train Loss: 0.0023, Val Loss: 0.0008\nValidation loss decreased (0.0030 --> 0.0008).  Saving model ...\nEpoch number2: Train Loss:0.0022889032452869562, Val Loss:0.0008070500878648473\nval accuracy: 1.0\nEpoch 3/12, Train Loss: 0.0014, Val Loss: 0.0006\nValidation loss decreased (0.0008 --> 0.0006).  Saving model ...\nEpoch number3: Train Loss:0.0014098693136093221, Val Loss:0.0005582524840009115\nval accuracy: 1.0\nEpoch 4/12, Train Loss: 0.0012, Val Loss: 0.0003\nValidation loss decreased (0.0006 --> 0.0003).  Saving model ...\nEpoch number4: Train Loss:0.0011576781543541927, Val Loss:0.0003003003675893669\nval accuracy: 1.0\nEpoch 5/12, Train Loss: 0.0006, Val Loss: 0.0002\nValidation loss decreased (0.0003 --> 0.0002).  Saving model ...\nEpoch number5: Train Loss:0.0006458299527414491, Val Loss:0.000193041981220231\nval accuracy: 1.0\nEpoch 6/12, Train Loss: 0.0006, Val Loss: 0.0002\nValidation loss decreased (0.0002 --> 0.0002).  Saving model ...\nEpoch number6: Train Loss:0.0005517629731403061, Val Loss:0.00015817585129193642\nval accuracy: 1.0\nEpoch 7/12, Train Loss: 0.0004, Val Loss: 0.0002\nValidation loss decreased (0.0002 --> 0.0002).  Saving model ...\nEpoch number7: Train Loss:0.00037564858697482407, Val Loss:0.0001572358515651646\nval accuracy: 1.0\nEpoch 8/12, Train Loss: 0.0003, Val Loss: 0.0001\nValidation loss decreased (0.0002 --> 0.0001).  Saving model ...\nEpoch number8: Train Loss:0.00028927761673758966, Val Loss:0.00011047386174918157\nval accuracy: 1.0\nEpoch 9/12, Train Loss: 0.0003, Val Loss: 0.0001\nValidation loss decreased (0.0001 --> 0.0001).  Saving model ...\nEpoch number9: Train Loss:0.00030105106204031336, Val Loss:0.00010469936350695084\nval accuracy: 1.0\nEpoch 10/12, Train Loss: 0.0003, Val Loss: 0.0001\nValidation loss decreased (0.0001 --> 0.0001).  Saving model ...\nEpoch number10: Train Loss:0.00029388940524861063, Val Loss:7.467105204224161e-05\nval accuracy: 1.0\nEpoch 11/12, Train Loss: 0.0002, Val Loss: 0.0001\nValidation loss decreased (0.0001 --> 0.0001).  Saving model ...\nEpoch number11: Train Loss:0.0002126618914656428, Val Loss:7.153637259262705e-05\nval accuracy: 1.0\nEpoch 12/12, Train Loss: 0.0002, Val Loss: 0.0001\nValidation loss decreased (0.0001 --> 0.0001).  Saving model ...\nEpoch number12: Train Loss:0.00017131861411618835, Val Loss:6.743015398037246e-05\ndecoding feature: shape\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 1.1277, saved_times: 16\nEpoch [200/300], Loss: 0.0000, saved_times: 24\nEpoch [300/300], Loss: 0.0000, saved_times: 92\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.59\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.4380, saved_times: 38\nEpoch [200/300], Loss: 0.3241, saved_times: 9\nEpoch [300/300], Loss: 0.2232, saved_times: 7\nBest model saved with loss: 0.2232\ndecoder train Accuracy for layer avgpool: 0.92\ntest Accuracy for layer avgpool: 0.44\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 1.7995, saved_times: 42\nEpoch [200/300], Loss: 1.7914, saved_times: 7\nEpoch [300/300], Loss: 1.7931, saved_times: 7\nBest model saved with loss: 1.7847\ndecoder train Accuracy for layer fc: 0.27\ntest Accuracy for layer fc: 0.21\ndecoding feature: color\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 2\nEpoch [200/300], Loss: 0.0000, saved_times: 0\nEpoch [300/300], Loss: 0.0000, saved_times: 0\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 1.00\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 96\nEpoch [200/300], Loss: 0.0000, saved_times: 85\nEpoch [300/300], Loss: 0.0000, saved_times: 76\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer avgpool: 1.00\ntest Accuracy for layer avgpool: 1.00\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 0.0003, saved_times: 100\nEpoch [200/300], Loss: 0.0000, saved_times: 88\nEpoch [300/300], Loss: 0.0000, saved_times: 81\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer fc: 1.00\ntest Accuracy for layer fc: 1.00\ndecoding feature: texture\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 11.1074, saved_times: 20\nEpoch [200/300], Loss: 4.6642, saved_times: 4\nEpoch [300/300], Loss: 3.5306, saved_times: 2\nBest model saved with loss: 0.0138\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.30\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.9872, saved_times: 36\nEpoch [200/300], Loss: 0.8156, saved_times: 10\nEpoch [300/300], Loss: 0.7498, saved_times: 6\nBest model saved with loss: 0.6845\ndecoder train Accuracy for layer avgpool: 0.70\ntest Accuracy for layer avgpool: 0.34\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 1.9267, saved_times: 15\nEpoch [200/300], Loss: 1.9224, saved_times: 2\nEpoch [300/300], Loss: 1.9205, saved_times: 1\nBest model saved with loss: 1.9181\ndecoder train Accuracy for layer fc: 0.20\ntest Accuracy for layer fc: 0.17\nnetwork model train target feature: texture\nDevice: cuda\ntraining network model for target feature: texture\nList of texture hold out classes: ['noise', 'hexgrid', 'rain']\n0-indexed dictionary of remaining items: {'solid': 0, 'stripes': 1, 'grid': 2, 'dots': 3, 'triangles': 4, 'zigzags': 5, 'pluses': 6}\nList of texture hold out classes: ['noise', 'hexgrid', 'rain']\n0-indexed dictionary of remaining items: {'solid': 0, 'stripes': 1, 'grid': 2, 'dots': 3, 'triangles': 4, 'zigzags': 5, 'pluses': 6}\ninf\nval accuracy: 0.4170868347338936\nEpoch 1/12, Train Loss: 1.2816, Val Loss: 3.0987\nValidation loss decreased (inf --> 3.0987).  Saving model ...\nEpoch number1: Train Loss:1.2815720161960702, Val Loss:3.0986793630263385\nval accuracy: 0.5310924369747899\nEpoch 2/12, Train Loss: 0.5969, Val Loss: 7.9563\nEpoch number2: Train Loss:0.5968522673793158, Val Loss:7.956320004503266\nval accuracy: 0.5240896358543418\nEpoch 3/12, Train Loss: 0.2792, Val Loss: 9.3298\nEpoch number3: Train Loss:0.2792266761422505, Val Loss:9.329772265351453\nval accuracy: 0.5680672268907563\nEpoch 4/12, Train Loss: 0.1854, Val Loss: 9.7295\nEpoch number4: Train Loss:0.18540398158762963, Val Loss:9.7295107454145\nval accuracy: 0.5596638655462185\nEpoch 5/12, Train Loss: 0.0985, Val Loss: 12.5474\nEpoch number5: Train Loss:0.09853959830352933, Val Loss:12.547383885957947\nval accuracy: 0.534733893557423\nEpoch 6/12, Train Loss: 0.0925, Val Loss: 16.6752\nEpoch number6: Train Loss:0.0924610479710401, Val Loss:16.675172338832994\nval accuracy: 0.5745098039215686\nEpoch 7/12, Train Loss: 0.1240, Val Loss: 12.0539\nEpoch number7: Train Loss:0.12402446954660444, Val Loss:12.053886912583637\nval accuracy: 0.5736694677871148\nEpoch 8/12, Train Loss: 0.0593, Val Loss: 11.9362\nEpoch number8: Train Loss:0.05932535131193111, Val Loss:11.936233500205502\nval accuracy: 0.5750700280112044\nEpoch 9/12, Train Loss: 0.0330, Val Loss: 13.1951\nEpoch number9: Train Loss:0.03303842995065989, Val Loss:13.195061377517316\nval accuracy: 0.5644257703081232\nEpoch 10/12, Train Loss: 0.0505, Val Loss: 11.6340\nEpoch number10: Train Loss:0.0505427684059296, Val Loss:11.633954631046755\nval accuracy: 0.55406162464986\nEpoch 11/12, Train Loss: 0.0608, Val Loss: 10.1450\nEpoch number11: Train Loss:0.0608005768502723, Val Loss:10.14504689128459\nval accuracy: 0.5742296918767507\nEpoch 12/12, Train Loss: 0.0246, Val Loss: 11.8100\nEpoch number12: Train Loss:0.02460667578375874, Val Loss:11.809969929286412\ndecoding feature: shape\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 57\nEpoch [200/300], Loss: 0.0000, saved_times: 99\nEpoch [300/300], Loss: 0.0000, saved_times: 30\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.79\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.2821, saved_times: 27\nEpoch [200/300], Loss: 0.2443, saved_times: 5\nEpoch [300/300], Loss: 0.3386, saved_times: 3\nBest model saved with loss: 0.1544\ndecoder train Accuracy for layer avgpool: 0.93\ntest Accuracy for layer avgpool: 0.75\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 1.7560, saved_times: 32\nEpoch [200/300], Loss: 1.7367, saved_times: 11\nEpoch [300/300], Loss: 1.7405, saved_times: 5\nBest model saved with loss: 1.7213\ndecoder train Accuracy for layer fc: 0.32\ntest Accuracy for layer fc: 0.25\ndecoding feature: color\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 75\nEpoch [200/300], Loss: 0.0000, saved_times: 67\nEpoch [300/300], Loss: 0.0000, saved_times: 8\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.79\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.0006, saved_times: 62\nEpoch [200/300], Loss: 0.0002, saved_times: 38\nEpoch [300/300], Loss: 0.0000, saved_times: 55\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer avgpool: 1.00\ntest Accuracy for layer avgpool: 0.82\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 0.5425, saved_times: 70\nEpoch [200/300], Loss: 0.4955, saved_times: 31\nEpoch [300/300], Loss: 0.4739, saved_times: 21\nBest model saved with loss: 0.4731\ndecoder train Accuracy for layer fc: 0.82\ntest Accuracy for layer fc: 0.62\ndecoding feature: texture\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nActivation of layer layer4: Shape=3430\nActivation of layer avgpool: Shape=3430\nActivation of layer fc: Shape=3430\nActivation of layer layer4: Shape=3570\nActivation of layer avgpool: Shape=3570\nActivation of layer fc: Shape=3570\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nlearning rate for layer4 = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 17\nEpoch [200/300], Loss: 0.0000, saved_times: 0\nEpoch [300/300], Loss: 0.0000, saved_times: 0\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer layer4: 1.00\ntest Accuracy for layer layer4: 0.62\nlearning rate for avgpool = 0.01\nEpoch [100/300], Loss: 0.6470, saved_times: 18\nEpoch [200/300], Loss: 0.5102, saved_times: 6\nEpoch [300/300], Loss: 0.3820, saved_times: 6\nBest model saved with loss: 0.3059\ndecoder train Accuracy for layer avgpool: 0.91\ntest Accuracy for layer avgpool: 0.66\nlearning rate for fc = 0.01\nEpoch [100/300], Loss: 1.1333, saved_times: 25\nEpoch [200/300], Loss: 1.1132, saved_times: 6\nEpoch [300/300], Loss: 1.1077, saved_times: 6\nBest model saved with loss: 1.1037\ndecoder train Accuracy for layer fc: 0.52\ntest Accuracy for layer fc: 0.52\ndecoding accuracies: {'shape': {'shape': {'layer4': (1.0, 0.9974789915966387), 'avgpool': (1.0, 0.9957983193277311), 'fc': (0.9851311953352769, 0.9921568627450981)}, 'color': {'layer4': (1.0, 0.6481792717086835), 'avgpool': (1.0, 0.8030812324929972), 'fc': (0.34402332361516036, 0.3299719887955182)}, 'texture': {'layer4': (0.9982507288629737, 0.25014005602240896), 'avgpool': (0.5938775510204082, 0.30644257703081235), 'fc': (0.1588921282798834, 0.1484593837535014)}}, 'untrained': {'shape': {'layer4': (1.0, 0.9484593837535014), 'avgpool': (0.9900874635568513, 0.9378151260504202), 'fc': (0.42244897959183675, 0.4103641456582633)}, 'color': {'layer4': (1.0, 0.9795518207282913), 'avgpool': (1.0, 0.9980392156862745), 'fc': (0.8303206997084548, 0.7994397759103642)}, 'texture': {'layer4': (1.0, 0.33865546218487397), 'avgpool': (0.6609329446064139, 0.3826330532212885), 'fc': (0.22040816326530613, 0.19439775910364146)}}, 'color': {'shape': {'layer4': (1.0, 0.5876750700280112), 'avgpool': (0.9160349854227405, 0.43781512605042017), 'fc': (0.2737609329446064, 0.21120448179271709)}, 'color': {'layer4': (1.0, 0.9997198879551821), 'avgpool': (1.0, 1.0), 'fc': (1.0, 1.0)}, 'texture': {'layer4': (0.9967930029154519, 0.2988795518207283), 'avgpool': (0.6953352769679301, 0.3389355742296919), 'fc': (0.19970845481049562, 0.1661064425770308)}}, 'texture': {'shape': {'layer4': (1.0, 0.7901960784313725), 'avgpool': (0.933527696793003, 0.750140056022409), 'fc': (0.32215743440233235, 0.24901960784313726)}, 'color': {'layer4': (1.0, 0.7924369747899159), 'avgpool': (1.0, 0.819047619047619), 'fc': (0.8233236151603499, 0.6221288515406163)}, 'texture': {'layer4': (1.0, 0.6162464985994398), 'avgpool': (0.9107871720116618, 0.6585434173669468), 'fc': (0.5177842565597668, 0.5176470588235295)}}}\n","output_type":"stream"}]}]}