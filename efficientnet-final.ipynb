{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8354058,"sourceType":"datasetVersion","datasetId":4963783},{"sourceId":8357344,"sourceType":"datasetVersion","datasetId":4966307}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install efficientnet_pytorch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-08T13:45:38.802747Z","iopub.execute_input":"2024-05-08T13:45:38.803422Z","iopub.status.idle":"2024-05-08T13:45:54.359225Z","shell.execute_reply.started":"2024-05-08T13:45:38.803392Z","shell.execute_reply":"2024-05-08T13:45:54.358044Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from efficientnet_pytorch) (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->efficientnet_pytorch) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\nBuilding wheels for collected packages: efficientnet_pytorch\n  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=852c66b538bb149a70b340b02494d513052d73e4a4f948ee0c92c8b919e2110d\n  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\nSuccessfully built efficientnet_pytorch\nInstalling collected packages: efficientnet_pytorch\nSuccessfully installed efficientnet_pytorch-0.7.1\n","output_type":"stream"}]},{"cell_type":"code","source":"#imports\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom efficientnet_pytorch import EfficientNet\nimport pandas as pd\nimport torchvision\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, RandomHorizontalFlip, RandomRotation, ColorJitter, RandomResizedCrop, RandomApply, RandomAffine\nfrom os.path import join\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom tqdm import tqdm\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:45:54.361795Z","iopub.execute_input":"2024-05-08T13:45:54.362185Z","iopub.status.idle":"2024-05-08T13:46:01.186275Z","shell.execute_reply.started":"2024-05-08T13:45:54.362143Z","shell.execute_reply":"2024-05-08T13:46:01.185495Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### define global variables","metadata":{}},{"cell_type":"code","source":"images_path = '/kaggle/input/trifeature/trifeature-dataset/color_texture_shape_stimuli/color_texture_shape_stimuli'","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.187302Z","iopub.execute_input":"2024-05-08T13:46:01.187697Z","iopub.status.idle":"2024-05-08T13:46:01.192401Z","shell.execute_reply.started":"2024-05-08T13:46:01.187673Z","shell.execute_reply":"2024-05-08T13:46:01.191413Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"version_network = 'version_0'\nversion_decoder = 'version_1'","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.194948Z","iopub.execute_input":"2024-05-08T13:46:01.195784Z","iopub.status.idle":"2024-05-08T13:46:01.204215Z","shell.execute_reply.started":"2024-05-08T13:46:01.195753Z","shell.execute_reply":"2024-05-08T13:46:01.203499Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"layer_names = ['_conv_head', '_avg_pooling', '_fc']","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.205453Z","iopub.execute_input":"2024-05-08T13:46:01.206035Z","iopub.status.idle":"2024-05-08T13:46:01.215538Z","shell.execute_reply.started":"2024-05-08T13:46:01.206006Z","shell.execute_reply":"2024-05-08T13:46:01.214612Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"markdown","source":"#### get path of labels mapping and log files","metadata":{}},{"cell_type":"code","source":"def get_labels_logs_path(feature_name, version):\n    if(feature_name == 'color'): \n        return '/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/color_splits/splits.pkl', f'/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/color_splits/logs/{version}_split.txt'\n    elif(feature_name == 'shape'): \n        return '/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/shape_splits/splits.pkl', f'/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/shape_splits/logs/{version}_split.txt'\n    elif(feature_name == 'texture'):\n        return '/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/texture_splits/splits.pkl', f'/kaggle/input/trifeature/trifeature-dataset/dataset_splits-20240426T140423Z-001/dataset_splits/texture_splits/logs/{version}_split.txt'\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.216517Z","iopub.execute_input":"2024-05-08T13:46:01.216850Z","iopub.status.idle":"2024-05-08T13:46:01.226001Z","shell.execute_reply.started":"2024-05-08T13:46:01.216819Z","shell.execute_reply":"2024-05-08T13:46:01.225151Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Extract truth labels as list for decoder y_test, y_train","metadata":{}},{"cell_type":"code","source":"def get_Truthlabels_list(feature_name, version, train_val):\n    decoder_labels_path, log_path = get_labels_logs_path(feature_name, version)\n    loaded_object = get_input_labels(decoder_labels_path)\n    loaded_object = loaded_object[version][train_val]\n    truth_labels = [item[feature_name] for item in loaded_object]\n    class_dict = get_class_labels_dict(log_path, feature_name)\n    labels_numeric = [class_dict[label] for label in truth_labels]\n    return labels_numeric","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.227054Z","iopub.execute_input":"2024-05-08T13:46:01.227297Z","iopub.status.idle":"2024-05-08T13:46:01.236512Z","shell.execute_reply.started":"2024-05-08T13:46:01.227276Z","shell.execute_reply":"2024-05-08T13:46:01.235755Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### extract labels as dictionary from pickle file","metadata":{}},{"cell_type":"code","source":"import pickle\n\n# loaded_object = {'version_0': {'train': [\n#    {'color': 'blue', 'shape': 'circle', 'texture': 'dots', 'exemplar':'0', 'fname': 'circle_dots_blue_0.png'},\n#     { }, { }]}}\n\ndef get_input_labels(path):\n    # Open the .pkl file for reading in binary mode\n    with open(path, 'rb') as f:\n        # Load the object from the file\n        loaded_object = pickle.load(f)\n#         list of dictionaries\n        return loaded_object","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.262617Z","iopub.execute_input":"2024-05-08T13:46:01.262922Z","iopub.status.idle":"2024-05-08T13:46:01.269457Z","shell.execute_reply.started":"2024-05-08T13:46:01.262900Z","shell.execute_reply":"2024-05-08T13:46:01.268757Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def get_classList(feature_name):\n    color_class = [\"red\", \"green\", \"blue\", \"yellow\", \"pink\", \"cyan\", \"purple\", \"ocean\", \"orange\", \"white\"]\n    shape_class = [\"triangle\", \"square\", \"plus\", \"circle\", \"tee\", \"rhombus\", \"pentagon\", \"star\", \"fivesquare\", \"trapezoid\"]\n    texture_class = [\"solid\", \"stripes\", \"grid\", \"hexgrid\", \"dots\", \"noise\", \"triangles\", \"zigzags\", \"rain\", \"pluses\"]\n    \n    if(feature_name == 'color'):\n        return color_class\n    elif(feature_name == 'shape'):\n        return shape_class\n    elif(feature_name == 'texture'):\n        return texture_class","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.270553Z","iopub.execute_input":"2024-05-08T13:46:01.270831Z","iopub.status.idle":"2024-05-08T13:46:01.279891Z","shell.execute_reply.started":"2024-05-08T13:46:01.270802Z","shell.execute_reply":"2024-05-08T13:46:01.279069Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_class_labels_dict(txt_file, feature_name):\n    with open(txt_file, 'r') as file:\n        content = file.readlines()\n\n    # Iterate through the lines to find the shape classes\n    classes = None\n    for line in content:\n        if feature_name in line:\n            # Extract the shape classes\n            classes = line.split(':')[1].strip()[1:-1].split(', ')\n            classes = [cls.strip().strip(\"'\") for cls in classes]\n#             break\n\n    # Print the list of shape classes\n    print(f\"List of {feature_name} hold out classes:\", classes)\n    \n    original_class_list = get_classList(feature_name)\n    # Remove shape classes from the original list\n    remaining_items = [item for item in original_class_list if item not in classes]\n\n    # Create a 0-indexed dictionary of remaining items\n    indexed_dict = {item: index for index, item in enumerate(remaining_items)}\n\n    # Print the indexed dictionary\n    print(\"0-indexed dictionary of remaining items:\", indexed_dict)\n    \n    return indexed_dict\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.280891Z","iopub.execute_input":"2024-05-08T13:46:01.281142Z","iopub.status.idle":"2024-05-08T13:46:01.290303Z","shell.execute_reply.started":"2024-05-08T13:46:01.281121Z","shell.execute_reply":"2024-05-08T13:46:01.289451Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train_val_labels_path - pickle file\ndef getAllDataLoders(feature_name, b_size=64, version = 'version_0', shuffle_train=True):\n    \n    train_val_labels_path, log_path = get_labels_logs_path(feature_name, version)\n    # get list of label mappings\n    \n    train_val_labels = get_input_labels(train_val_labels_path)[version]\n    train_labels = train_val_labels['train']\n    val_labels = train_val_labels['val']\n    \n    # create dataset\n    train_dataset = getDataset(train_labels, log_path, feature_name  )\n    val_dataset = getDataset(val_labels, log_path, feature_name )\n\n    train_loader = DataLoader(train_dataset, batch_size=b_size, shuffle = shuffle_train)\n    val_loader = DataLoader(val_dataset, batch_size=b_size, shuffle=False)\n\n    \n    return train_loader, val_loader\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.291353Z","iopub.execute_input":"2024-05-08T13:46:01.291929Z","iopub.status.idle":"2024-05-08T13:46:01.304033Z","shell.execute_reply.started":"2024-05-08T13:46:01.291900Z","shell.execute_reply":"2024-05-08T13:46:01.303218Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def getDataset(labels, txt_file, feature_name):\n    \n    classes_dict = get_class_labels_dict(txt_file, feature_name)\n    dataset = TrifeatureDataset(images_path, labels, classes_dict, feature_name)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.305113Z","iopub.execute_input":"2024-05-08T13:46:01.305443Z","iopub.status.idle":"2024-05-08T13:46:01.314141Z","shell.execute_reply.started":"2024-05-08T13:46:01.305414Z","shell.execute_reply":"2024-05-08T13:46:01.313279Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Define Model","metadata":{}},{"cell_type":"code","source":"# device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.315175Z","iopub.execute_input":"2024-05-08T13:46:01.315416Z","iopub.status.idle":"2024-05-08T13:46:01.352178Z","shell.execute_reply.started":"2024-05-08T13:46:01.315396Z","shell.execute_reply":"2024-05-08T13:46:01.351219Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def getModel(num_classes=7):\n    \n    # Check if CUDA is available\n    print(\"Device:\", device)\n\n    # Set random seed for reproducibility\n    torch.manual_seed(42)\n \n    model = EfficientNet.from_name('efficientnet-b0', num_classes=num_classes)\n\n    model = model.to(device)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.353415Z","iopub.execute_input":"2024-05-08T13:46:01.353691Z","iopub.status.idle":"2024-05-08T13:46:01.365632Z","shell.execute_reply.started":"2024-05-08T13:46:01.353668Z","shell.execute_reply":"2024-05-08T13:46:01.364898Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = getModel()\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.366578Z","iopub.execute_input":"2024-05-08T13:46:01.366851Z","iopub.status.idle":"2024-05-08T13:46:01.651225Z","shell.execute_reply.started":"2024-05-08T13:46:01.366829Z","shell.execute_reply":"2024-05-08T13:46:01.650246Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Device: cuda\nEfficientNet(\n  (_conv_stem): Conv2dStaticSamePadding(\n    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n    (static_padding): ZeroPad2d((0, 1, 0, 1))\n  )\n  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_blocks): ModuleList(\n    (0): MBConvBlock(\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n        (static_padding): ZeroPad2d((1, 1, 1, 1))\n      )\n      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        32, 8, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        8, 32, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (1): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n        (static_padding): ZeroPad2d((0, 1, 0, 1))\n      )\n      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        96, 4, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        4, 96, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (2): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n        (static_padding): ZeroPad2d((1, 1, 1, 1))\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (3): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n        (static_padding): ZeroPad2d((1, 2, 1, 2))\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (4): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n        (static_padding): ZeroPad2d((2, 2, 2, 2))\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (5): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n        (static_padding): ZeroPad2d((0, 1, 0, 1))\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (6-7): 2 x MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n        (static_padding): ZeroPad2d((1, 1, 1, 1))\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (8): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n        (static_padding): ZeroPad2d((2, 2, 2, 2))\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (9-10): 2 x MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n        (static_padding): ZeroPad2d((2, 2, 2, 2))\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (11): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n        (static_padding): ZeroPad2d((1, 2, 1, 2))\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (12-14): 3 x MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d((2, 2, 2, 2))\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (15): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n        (static_padding): ZeroPad2d((1, 1, 1, 1))\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n  )\n  (_conv_head): Conv2dStaticSamePadding(\n    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n    (static_padding): Identity()\n  )\n  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n  (_dropout): Dropout(p=0.2, inplace=False)\n  (_fc): Linear(in_features=1280, out_features=7, bias=True)\n  (_swish): MemoryEfficientSwish()\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Make Dataset\n* requires classes_dict of 7 labels of target feature","metadata":{}},{"cell_type":"code","source":"class TrifeatureDataset(torch.utils.data.Dataset):\n#  details list - maps image to its actual labels\n#  classes_dict - dictionary of 7 classes of the target feature\n# ex - 'red':0, 'blue':1  this helps to map colour to a index for final output layer\n    def __init__(self, img_path, details_list, classes_dict, feature):\n        super(TrifeatureDataset, self).__init__()\n\n        self.img_path = img_path\n        self.details_list = details_list\n        self.feature = feature\n\n        self.transform = self._transform(224)\n        \n        self.classes = classes_dict\n\n\n    @staticmethod    \n    def _convert_image_to_rgb(image):\n        return image.convert(\"RGB\")\n\n    def _transform(self, n_px):\n        mean = [0.50190921, 0.50194219, 0.49818846]\n        std =  [0.1426835,  0.1282568,  0.13595397]\n        return Compose([\n            Resize(n_px),\n            self._convert_image_to_rgb,\n            ToTensor(),\n            Normalize(mean, std)\n        ])\n\n    def read_img(self, file_name):\n        im_path = join(self.img_path,file_name)   \n        img = Image.open(im_path)\n        img = self.transform(img)\n        return img\n\n    def __getitem__(self, index):\n        file_name = self.details_list[index]['fname']\n        img = self.read_img(file_name)\n        target_label = self.details_list[index][self.feature]\n        return img, self.classes[target_label]\n\n\n    def __len__(self):\n        return len(self.details_list)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.652340Z","iopub.execute_input":"2024-05-08T13:46:01.652729Z","iopub.status.idle":"2024-05-08T13:46:01.663144Z","shell.execute_reply.started":"2024-05-08T13:46:01.652679Z","shell.execute_reply":"2024-05-08T13:46:01.662171Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### make dataset","metadata":{}},{"cell_type":"code","source":"def getDataLoader(dataset, batch_size=64, shuffle = True):\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n    return data_loader","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.664348Z","iopub.execute_input":"2024-05-08T13:46:01.664961Z","iopub.status.idle":"2024-05-08T13:46:01.672596Z","shell.execute_reply.started":"2024-05-08T13:46:01.664930Z","shell.execute_reply":"2024-05-08T13:46:01.671762Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### Train Model","metadata":{}},{"cell_type":"code","source":"best_val_path = 'best_val.pth'\nbest_val_loss = float('inf')\nbest_train_loss = float('inf')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.683960Z","iopub.execute_input":"2024-05-08T13:46:01.684189Z","iopub.status.idle":"2024-05-08T13:46:01.696487Z","shell.execute_reply.started":"2024-05-08T13:46:01.684169Z","shell.execute_reply":"2024-05-08T13:46:01.695761Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, criterion, best_val_loss=float('inf'), num_epochs=30):\n    print(best_val_loss)\n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n\n        for images, labels in train_loader:\n#             print(type(images))\n            images, labels = images.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_train_loss += loss.item() * images.size(0)\n\n        avg_train_loss = total_train_loss / len(train_loader.dataset)\n\n        # Validation phase\n        model.eval()\n        total_val_loss = 0\n        correct_predictions = 0\n        total_predictions = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                total_val_loss += loss.item() * images.size(0)\n                \n                _, predicted = torch.max(outputs.data, 1)\n                total_predictions += labels.size(0)\n                correct_predictions += (predicted == labels).sum().item()\n\n        avg_val_loss = total_val_loss / len(val_loader.dataset)\n        val_accuracy = correct_predictions / total_predictions\n        print(\"val accuracy:\", val_accuracy)\n\n        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n        \n        \n        # Save the model if validation loss has decreased\n        if avg_val_loss < best_val_loss:\n            print('Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...'.format(\n                best_val_loss,\n                avg_val_loss))\n            torch.save(model.state_dict(), best_val_path)\n            best_val_loss = avg_val_loss\n        print(f\"Epoch number{epoch+1}: Train Loss:{avg_train_loss}, Val Loss:{avg_val_loss}\")\n    return best_val_loss\n        \n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.697617Z","iopub.execute_input":"2024-05-08T13:46:01.698089Z","iopub.status.idle":"2024-05-08T13:46:01.709937Z","shell.execute_reply.started":"2024-05-08T13:46:01.698065Z","shell.execute_reply":"2024-05-08T13:46:01.709069Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"### Get Embeddings from intermediate layers","metadata":{}},{"cell_type":"code","source":"# Hook function to store activations\ndef hook_fn(module, input, output, name, activations):\n    if name not in activations:\n        activations[name] = []\n#     print(\"Type of activations is\", type(activations[name]))\n    activations[name].append(output)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.725462Z","iopub.execute_input":"2024-05-08T13:46:01.725734Z","iopub.status.idle":"2024-05-08T13:46:01.734742Z","shell.execute_reply.started":"2024-05-08T13:46:01.725691Z","shell.execute_reply":"2024-05-08T13:46:01.733816Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"\ndef getEmbeddings(model, input_dataloader , layer_names = ['avgpool', 'fc']):\n    # Define the layers from which you want to extract activations\n    # layer_names = ['_conv_head', '_fc']\n\n    # Dictionary to store the activations of selected layers\n    activations = {}\n    hook_handles = {}\n\n    # Register hooks on the selected layers\n    for name, module in model.named_modules():\n        if name in layer_names:\n            handle = module.register_forward_hook(lambda m, i, o, name=name: hook_fn(m, i, o, name, activations))\n            hook_handles[name] = handle\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Forward pass\n    with torch.no_grad():\n        for images, labels in input_dataloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            \n    for key, val in hook_handles.items():\n        hook_handles[key].remove()\n\n    # Extract and print activations\n    for name, activation in activations.items():\n        print(f'Activation of layer {name}: Shape={len(activation)}')\n        \n    return activations","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:01.735816Z","iopub.execute_input":"2024-05-08T13:46:01.736097Z","iopub.status.idle":"2024-05-08T13:46:01.744775Z","shell.execute_reply.started":"2024-05-08T13:46:01.736075Z","shell.execute_reply":"2024-05-08T13:46:01.743917Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"## Defining a neural network for Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"## Train Decoder","metadata":{}},{"cell_type":"code","source":"class LogisticRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        return self.linear(x)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:46:58.494835Z","iopub.execute_input":"2024-05-08T13:46:58.495253Z","iopub.status.idle":"2024-05-08T13:46:58.500805Z","shell.execute_reply.started":"2024-05-08T13:46:58.495224Z","shell.execute_reply":"2024-05-08T13:46:58.499852Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class DecoderDataset(Dataset):\n    def __init__(self, inputs, labels):\n        self.inputs = inputs\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, idx):\n        input_data = torch.tensor(self.inputs[idx], dtype=torch.float)\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return input_data, label\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:47:01.244492Z","iopub.execute_input":"2024-05-08T13:47:01.245384Z","iopub.status.idle":"2024-05-08T13:47:01.251500Z","shell.execute_reply.started":"2024-05-08T13:47:01.245351Z","shell.execute_reply":"2024-05-08T13:47:01.250505Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def decoder_accuracy(model, dataloader=None, embeddings=None, labels=None):\n    \n    if(not dataloader):\n        np_list = [np.array(tensor.cpu()) for tensor in embeddings]\n        embeddings_np = np.array(np_list)\n        embeddings_np = embeddings_np.reshape(embeddings_np.shape[0], -1)\n        input_dim = embeddings_np.shape[1]\n        output_dim = 7\n        dataset = DecoderDataset(embeddings_np, labels)\n\n        dataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n    \n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs.float())\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:47:03.792320Z","iopub.execute_input":"2024-05-08T13:47:03.792898Z","iopub.status.idle":"2024-05-08T13:47:03.801111Z","shell.execute_reply.started":"2024-05-08T13:47:03.792869Z","shell.execute_reply":"2024-05-08T13:47:03.800156Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"best_decoder_path = 'best_decoder.pth'","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:47:07.830568Z","iopub.execute_input":"2024-05-08T13:47:07.831506Z","iopub.status.idle":"2024-05-08T13:47:07.836099Z","shell.execute_reply.started":"2024-05-08T13:47:07.831468Z","shell.execute_reply":"2024-05-08T13:47:07.834947Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def train_custom_decoder(train_embeddings, train_labels, lr=0.001, num_epochs=200):\n    \n    np_list = [np.array(tensor.cpu()) for tensor in train_embeddings]\n    train_embeddings_np = np.array(np_list)\n    train_embeddings_np = train_embeddings_np.reshape(train_embeddings_np.shape[0], -1)\n    input_dim = train_embeddings_np.shape[1]\n    output_dim = 7\n    dataset = DecoderDataset(train_embeddings_np, train_labels)\n    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n    \n    model = LogisticRegression(input_dim, output_dim)\n    model = model.to(device)\n    \n    best_loss = float('inf')\n    criterion = nn.CrossEntropyLoss()\n#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n    optimizer = optim.Adam(model.parameters(), lr= lr)\n\n    count=0\n\n    for epoch in range(num_epochs):\n        epoch_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs.float())\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item() * inputs.size(0)\n        \n        epoch_loss /= len(train_loader.dataset)  # Compute average epoch loss\n        if epoch_loss < best_loss:\n#             print(\"saving model\")\n            count+=1\n            best_loss = epoch_loss\n            torch.save(model.state_dict(), best_decoder_path)\n        \n        if (epoch+1) % 100 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, saved_times: {count}')\n            count=0\n    print(f\"Best model saved with loss: {best_loss:.4f}\")\n    model.load_state_dict(torch.load(best_decoder_path))\n    return model, decoder_accuracy(model, dataloader = train_loader)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:47:37.912757Z","iopub.execute_input":"2024-05-08T13:47:37.913143Z","iopub.status.idle":"2024-05-08T13:47:37.924591Z","shell.execute_reply.started":"2024-05-08T13:47:37.913116Z","shell.execute_reply":"2024-05-08T13:47:37.923665Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Decoder Test accuracy","metadata":{}},{"cell_type":"markdown","source":"# whole pipeline:\n* train original network on:\n    shape, color, texture\n  also take untrained network\n* for each of the above networks train and test decoder independently on the features:\n    shape, color, texture","metadata":{}},{"cell_type":"code","source":"learning_rate = {'_conv_head':0.01 , '_avg_pooling': 0.01, '_fc': 0.01}","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:47:44.034135Z","iopub.execute_input":"2024-05-08T13:47:44.034498Z","iopub.status.idle":"2024-05-08T13:47:44.038919Z","shell.execute_reply.started":"2024-05-08T13:47:44.034471Z","shell.execute_reply":"2024-05-08T13:47:44.037865Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def whole_pipeline(target_feature, train_embeddings=None, test_embeddings=None):\n\n    model = getModel()\n    # Define the loss criterion\n    criterion = nn.CrossEntropyLoss()\n    # Define the optimizer\n    if(target_feature != 'untrained'):\n        lr=0.0001\n        if(target_feature == 'texture'):\n            lr=0.001\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0001)\n\n        print(\"training network model for target feature:\", target_feature)\n        train_val_labels_path = get_labels_logs_path(target_feature, version_network)\n        train_loader, val_loader = getAllDataLoders(target_feature) \n        train_model(model, train_loader, val_loader, optimizer, criterion, best_val_loss, num_epochs=20)\n        model.load_state_dict(torch.load('best_val.pth'))\n        \n        \n    # find embeddings for version 1\n    features = ['shape', 'color', 'texture']\n    accuracies = {}\n    \n    for feature_name in features:\n\n        print(\"decoding feature:\", feature_name)\n        decoder_train_loader, decoder_val_loader = getAllDataLoders( feature_name, 1, version = version_decoder, shuffle_train=False)       \n        train_embeddings = getEmbeddings(model, decoder_train_loader , layer_names = layer_names)\n        test_embeddings = getEmbeddings(model, decoder_val_loader , layer_names = layer_names)\n\n\n        train_labels = get_Truthlabels_list(feature_name, version_decoder, 'train')        \n        \n        \n        test_labels = get_Truthlabels_list(feature_name, version_decoder, 'val')\n\n        layer_accuracies = {}\n        for layer in layer_names:\n            print(f\"learning rate for {layer} =\", learning_rate[layer])\n            \n            decoder, decoder_acc_train = train_custom_decoder(train_embeddings[layer] , train_labels, learning_rate[layer], num_epochs=300)\n            \n            print(f\"decoder train Accuracy for layer {layer}: {decoder_acc_train:.2f}\")\n            # Predict on the test set\n            decoder_acc_test = decoder_accuracy(decoder, embeddings = test_embeddings[layer], labels = test_labels)\n            print(f\"test Accuracy for layer {layer}: {decoder_acc_test:.2f}\")\n            layer_accuracies[layer] = (decoder_acc_train, decoder_acc_test)\n        accuracies[feature_name] = layer_accuracies\n\n    return accuracies, train_embeddings\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:49:23.201426Z","iopub.execute_input":"2024-05-08T13:49:23.201895Z","iopub.status.idle":"2024-05-08T13:49:23.214202Z","shell.execute_reply.started":"2024-05-08T13:49:23.201855Z","shell.execute_reply":"2024-05-08T13:49:23.213228Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"embeddings_path = 'embeddings_file.pkl'\ndecoding_acc_path = 'decoding_acc.pkl'","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:49:34.241124Z","iopub.execute_input":"2024-05-08T13:49:34.242096Z","iopub.status.idle":"2024-05-08T13:49:34.246053Z","shell.execute_reply.started":"2024-05-08T13:49:34.242060Z","shell.execute_reply":"2024-05-08T13:49:34.245013Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def main():\n    target_features = [ 'shape', 'untrained', 'color', 'texture']\n    \n    model_accuracies = {} \n    embeddings = {}\n#     model_accuracies = {'target_feature': {'feature_name': {'layer_name': accuracy_value}}}\n    for target_feature in target_features:\n        # get accuracy of current model for all features\n        print(\"network model train target feature:\", target_feature)\n        accuracy_dict, emb = whole_pipeline(target_feature)\n#         train_embeddings, test_embeddings = whole_pipeline(target_feature)\n#         return train_embeddings, test_embeddings\n        model_accuracies[target_feature] = accuracy_dict\n        embeddings[target_feature] = emb\n        \n    with open(embeddings_path, 'wb') as f:\n        pickle.dump(embeddings, f)\n    \n    with open(decoding_acc_path, 'wb') as f:\n        pickle.dump(model_accuracies, f)\n    print(\"decoding accuracies:\", model_accuracies)\n    return model_accuracies\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:47:50.872852Z","iopub.execute_input":"2024-05-08T13:47:50.873205Z","iopub.status.idle":"2024-05-08T13:47:50.880536Z","shell.execute_reply.started":"2024-05-08T13:47:50.873178Z","shell.execute_reply":"2024-05-08T13:47:50.879607Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model_acc = main()","metadata":{"execution":{"iopub.status.busy":"2024-05-08T13:49:45.911807Z","iopub.execute_input":"2024-05-08T13:49:45.912162Z","iopub.status.idle":"2024-05-08T15:33:52.845881Z","shell.execute_reply.started":"2024-05-08T13:49:45.912135Z","shell.execute_reply":"2024-05-08T15:33:52.844871Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"network model train target feature: shape\nDevice: cuda\ntraining network model for target feature: shape\nList of shape hold out classes: ['trapezoid', 'pentagon', 'square']\n0-indexed dictionary of remaining items: {'triangle': 0, 'plus': 1, 'circle': 2, 'tee': 3, 'rhombus': 4, 'star': 5, 'fivesquare': 6}\nList of shape hold out classes: ['trapezoid', 'pentagon', 'square']\n0-indexed dictionary of remaining items: {'triangle': 0, 'plus': 1, 'circle': 2, 'tee': 3, 'rhombus': 4, 'star': 5, 'fivesquare': 6}\ninf\nval accuracy: 0.14285714285714285\nEpoch 1/20, Train Loss: 1.8048, Val Loss: 1.9463\nValidation loss decreased (inf --> 1.9463).  Saving model ...\nEpoch number1: Train Loss:1.8048245525221087, Val Loss:1.9463359486822989\nval accuracy: 0.14285714285714285\nEpoch 2/20, Train Loss: 1.1545, Val Loss: 1.9507\nEpoch number2: Train Loss:1.154523030175412, Val Loss:1.950733352041378\nval accuracy: 0.14285714285714285\nEpoch 3/20, Train Loss: 0.5879, Val Loss: 1.9904\nEpoch number3: Train Loss:0.5878938040649926, Val Loss:1.990393314441713\nval accuracy: 0.14285714285714285\nEpoch 4/20, Train Loss: 0.2961, Val Loss: 2.2023\nEpoch number4: Train Loss:0.29611383534033864, Val Loss:2.202273052279689\nval accuracy: 0.14285714285714285\nEpoch 5/20, Train Loss: 0.1675, Val Loss: 3.0964\nEpoch number5: Train Loss:0.16750903185185467, Val Loss:3.096411347456005\nval accuracy: 0.14285714285714285\nEpoch 6/20, Train Loss: 0.0974, Val Loss: 5.3287\nEpoch number6: Train Loss:0.09743770108104795, Val Loss:5.3287145539826035\nval accuracy: 0.14285714285714285\nEpoch 7/20, Train Loss: 0.0914, Val Loss: 8.1366\nEpoch number7: Train Loss:0.09142102436664848, Val Loss:8.136603738480256\nval accuracy: 0.19831932773109243\nEpoch 8/20, Train Loss: 0.0564, Val Loss: 5.6499\nEpoch number8: Train Loss:0.056406928690112365, Val Loss:5.649908225810161\nval accuracy: 0.3543417366946779\nEpoch 9/20, Train Loss: 0.0535, Val Loss: 3.3450\nEpoch number9: Train Loss:0.053467314423173234, Val Loss:3.3449674490119228\nval accuracy: 0.5243697478991597\nEpoch 10/20, Train Loss: 0.0394, Val Loss: 1.7189\nValidation loss decreased (1.9463 --> 1.7189).  Saving model ...\nEpoch number10: Train Loss:0.03943098162249296, Val Loss:1.7188824134714462\nval accuracy: 0.82296918767507\nEpoch 11/20, Train Loss: 0.0314, Val Loss: 0.4533\nValidation loss decreased (1.7189 --> 0.4533).  Saving model ...\nEpoch number11: Train Loss:0.031409106970799995, Val Loss:0.4533404560149217\nval accuracy: 0.8131652661064426\nEpoch 12/20, Train Loss: 0.0362, Val Loss: 0.5435\nEpoch number12: Train Loss:0.0362041409352778, Val Loss:0.5434639232165339\nval accuracy: 0.9\nEpoch 13/20, Train Loss: 0.0249, Val Loss: 0.4780\nEpoch number13: Train Loss:0.024894416869859652, Val Loss:0.4779627333168222\nval accuracy: 0.9221288515406163\nEpoch 14/20, Train Loss: 0.0165, Val Loss: 0.2256\nValidation loss decreased (0.4533 --> 0.2256).  Saving model ...\nEpoch number14: Train Loss:0.016521245034264928, Val Loss:0.22556928803606863\nval accuracy: 0.9588235294117647\nEpoch 15/20, Train Loss: 0.0153, Val Loss: 0.1527\nValidation loss decreased (0.2256 --> 0.1527).  Saving model ...\nEpoch number15: Train Loss:0.015328843750021592, Val Loss:0.15272758590156624\nval accuracy: 0.9322128851540616\nEpoch 16/20, Train Loss: 0.0146, Val Loss: 0.2210\nEpoch number16: Train Loss:0.014637861722325445, Val Loss:0.22103679290338724\nval accuracy: 0.8969187675070028\nEpoch 17/20, Train Loss: 0.0143, Val Loss: 0.2749\nEpoch number17: Train Loss:0.01428366346127264, Val Loss:0.274920834162656\nval accuracy: 0.9126050420168067\nEpoch 18/20, Train Loss: 0.0162, Val Loss: 0.5014\nEpoch number18: Train Loss:0.016232446505098926, Val Loss:0.5013863857386827\nval accuracy: 0.9537815126050421\nEpoch 19/20, Train Loss: 0.0172, Val Loss: 0.2027\nEpoch number19: Train Loss:0.017151209082096032, Val Loss:0.2027219883492347\nval accuracy: 0.9358543417366947\nEpoch 20/20, Train Loss: 0.0181, Val Loss: 0.1898\nEpoch number20: Train Loss:0.0180590624874865, Val Loss:0.18981494935704213\ndecoding feature: shape\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 8\nEpoch [200/300], Loss: 0.0000, saved_times: 0\nEpoch [300/300], Loss: 0.0000, saved_times: 0\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer _conv_head: 1.00\ntest Accuracy for layer _conv_head: 0.99\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 0.0159, saved_times: 14\nEpoch [200/300], Loss: 0.0088, saved_times: 10\nEpoch [300/300], Loss: 0.0025, saved_times: 4\nBest model saved with loss: 0.0017\ndecoder train Accuracy for layer _avg_pooling: 1.00\ntest Accuracy for layer _avg_pooling: 0.99\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 0.0770, saved_times: 51\nEpoch [200/300], Loss: 0.0757, saved_times: 14\nEpoch [300/300], Loss: 0.0705, saved_times: 4\nBest model saved with loss: 0.0687\ndecoder train Accuracy for layer _fc: 0.97\ntest Accuracy for layer _fc: 0.99\ndecoding feature: color\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 21.3510, saved_times: 26\nEpoch [200/300], Loss: 10.9631, saved_times: 5\nEpoch [300/300], Loss: 3.5546, saved_times: 3\nBest model saved with loss: 0.2118\ndecoder train Accuracy for layer _conv_head: 0.99\ntest Accuracy for layer _conv_head: 0.44\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.0436, saved_times: 28\nEpoch [200/300], Loss: 0.8473, saved_times: 7\nEpoch [300/300], Loss: 0.7068, saved_times: 5\nBest model saved with loss: 0.6701\ndecoder train Accuracy for layer _avg_pooling: 0.74\ntest Accuracy for layer _avg_pooling: 0.36\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.8627, saved_times: 15\nEpoch [200/300], Loss: 1.8644, saved_times: 1\nEpoch [300/300], Loss: 1.8750, saved_times: 0\nBest model saved with loss: 1.8615\ndecoder train Accuracy for layer _fc: 0.22\ntest Accuracy for layer _fc: 0.17\ndecoding feature: texture\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 18.7138, saved_times: 22\nEpoch [200/300], Loss: 2.0280, saved_times: 9\nEpoch [300/300], Loss: 7.8601, saved_times: 3\nBest model saved with loss: 0.0647\ndecoder train Accuracy for layer _conv_head: 1.00\ntest Accuracy for layer _conv_head: 0.22\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.9254, saved_times: 17\nEpoch [200/300], Loss: 1.7725, saved_times: 5\nEpoch [300/300], Loss: 1.6921, saved_times: 5\nBest model saved with loss: 1.5880\ndecoder train Accuracy for layer _avg_pooling: 0.44\ntest Accuracy for layer _avg_pooling: 0.22\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.9628, saved_times: 8\nEpoch [200/300], Loss: 1.9596, saved_times: 1\nEpoch [300/300], Loss: 1.9585, saved_times: 1\nBest model saved with loss: 1.9510\ndecoder train Accuracy for layer _fc: 0.18\ntest Accuracy for layer _fc: 0.16\nnetwork model train target feature: untrained\nDevice: cuda\ndecoding feature: shape\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 1.9473, saved_times: 4\nEpoch [200/300], Loss: 1.9467, saved_times: 0\nEpoch [300/300], Loss: 1.9472, saved_times: 1\nBest model saved with loss: 1.9464\ndecoder train Accuracy for layer _conv_head: 0.14\ntest Accuracy for layer _conv_head: 0.14\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.9469, saved_times: 7\nEpoch [200/300], Loss: 1.9467, saved_times: 1\nEpoch [300/300], Loss: 1.9471, saved_times: 0\nBest model saved with loss: 1.9465\ndecoder train Accuracy for layer _avg_pooling: 0.14\ntest Accuracy for layer _avg_pooling: 0.14\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.9468, saved_times: 6\nEpoch [200/300], Loss: 1.9477, saved_times: 2\nEpoch [300/300], Loss: 1.9470, saved_times: 0\nBest model saved with loss: 1.9465\ndecoder train Accuracy for layer _fc: 0.14\ntest Accuracy for layer _fc: 0.14\ndecoding feature: color\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 1.9469, saved_times: 5\nEpoch [200/300], Loss: 1.9470, saved_times: 1\nEpoch [300/300], Loss: 1.9470, saved_times: 0\nBest model saved with loss: 1.9465\ndecoder train Accuracy for layer _conv_head: 0.14\ntest Accuracy for layer _conv_head: 0.14\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.9470, saved_times: 9\nEpoch [200/300], Loss: 1.9468, saved_times: 0\nEpoch [300/300], Loss: 1.9472, saved_times: 0\nBest model saved with loss: 1.9464\ndecoder train Accuracy for layer _avg_pooling: 0.14\ntest Accuracy for layer _avg_pooling: 0.14\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.9473, saved_times: 5\nEpoch [200/300], Loss: 1.9466, saved_times: 0\nEpoch [300/300], Loss: 1.9474, saved_times: 0\nBest model saved with loss: 1.9465\ndecoder train Accuracy for layer _fc: 0.14\ntest Accuracy for layer _fc: 0.14\ndecoding feature: texture\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 1.9470, saved_times: 5\nEpoch [200/300], Loss: 1.9471, saved_times: 0\nEpoch [300/300], Loss: 1.9474, saved_times: 0\nBest model saved with loss: 1.9465\ndecoder train Accuracy for layer _conv_head: 0.14\ntest Accuracy for layer _conv_head: 0.14\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.9473, saved_times: 5\nEpoch [200/300], Loss: 1.9467, saved_times: 2\nEpoch [300/300], Loss: 1.9473, saved_times: 0\nBest model saved with loss: 1.9465\ndecoder train Accuracy for layer _avg_pooling: 0.14\ntest Accuracy for layer _avg_pooling: 0.14\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.9472, saved_times: 9\nEpoch [200/300], Loss: 1.9470, saved_times: 0\nEpoch [300/300], Loss: 1.9474, saved_times: 0\nBest model saved with loss: 1.9465\ndecoder train Accuracy for layer _fc: 0.14\ntest Accuracy for layer _fc: 0.14\nnetwork model train target feature: color\nDevice: cuda\ntraining network model for target feature: color\nList of color hold out classes: ['orange', 'blue', 'purple']\n0-indexed dictionary of remaining items: {'red': 0, 'green': 1, 'yellow': 2, 'pink': 3, 'cyan': 4, 'ocean': 5, 'white': 6}\nList of color hold out classes: ['orange', 'blue', 'purple']\n0-indexed dictionary of remaining items: {'red': 0, 'green': 1, 'yellow': 2, 'pink': 3, 'cyan': 4, 'ocean': 5, 'white': 6}\ninf\nval accuracy: 0.14285714285714285\nEpoch 1/20, Train Loss: 1.1854, Val Loss: 1.9485\nValidation loss decreased (inf --> 1.9485).  Saving model ...\nEpoch number1: Train Loss:1.185430592194243, Val Loss:1.9484940323843007\nval accuracy: 0.14285714285714285\nEpoch 2/20, Train Loss: 0.2332, Val Loss: 1.9620\nEpoch number2: Train Loss:0.23315952017946773, Val Loss:1.9619636360002832\nval accuracy: 0.14285714285714285\nEpoch 3/20, Train Loss: 0.0793, Val Loss: 1.9903\nEpoch number3: Train Loss:0.07930712936332031, Val Loss:1.9902767419147225\nval accuracy: 0.14285714285714285\nEpoch 4/20, Train Loss: 0.0478, Val Loss: 2.0871\nEpoch number4: Train Loss:0.04782552688705678, Val Loss:2.0871398869682762\nval accuracy: 0.14285714285714285\nEpoch 5/20, Train Loss: 0.0257, Val Loss: 2.3595\nEpoch number5: Train Loss:0.025692194435235016, Val Loss:2.359452478959113\nval accuracy: 0.14285714285714285\nEpoch 6/20, Train Loss: 0.0188, Val Loss: 2.9828\nEpoch number6: Train Loss:0.01877175382563828, Val Loss:2.9827646604105205\nval accuracy: 0.14285714285714285\nEpoch 7/20, Train Loss: 0.0285, Val Loss: 3.5034\nEpoch number7: Train Loss:0.028463642559836973, Val Loss:3.503366716635995\nval accuracy: 0.30056022408963584\nEpoch 8/20, Train Loss: 0.0224, Val Loss: 4.5987\nEpoch number8: Train Loss:0.022354502534987976, Val Loss:4.5987124822386845\nval accuracy: 0.5593837535014006\nEpoch 9/20, Train Loss: 0.0147, Val Loss: 2.0324\nEpoch number9: Train Loss:0.01471017421462163, Val Loss:2.032394326567984\nval accuracy: 0.9226890756302522\nEpoch 10/20, Train Loss: 0.0104, Val Loss: 0.2557\nValidation loss decreased (1.9485 --> 0.2557).  Saving model ...\nEpoch number10: Train Loss:0.010372760226140912, Val Loss:0.25565635266841624\nval accuracy: 0.9722689075630252\nEpoch 11/20, Train Loss: 0.0056, Val Loss: 0.0785\nValidation loss decreased (0.2557 --> 0.0785).  Saving model ...\nEpoch number11: Train Loss:0.005606388130141069, Val Loss:0.07848285856847169\nval accuracy: 0.996078431372549\nEpoch 12/20, Train Loss: 0.0082, Val Loss: 0.0158\nValidation loss decreased (0.0785 --> 0.0158).  Saving model ...\nEpoch number12: Train Loss:0.008168998006161074, Val Loss:0.015839511951374893\nval accuracy: 0.9946778711484594\nEpoch 13/20, Train Loss: 0.0063, Val Loss: 0.0161\nEpoch number13: Train Loss:0.006349861853872765, Val Loss:0.01605123622076852\nval accuracy: 0.9983193277310924\nEpoch 14/20, Train Loss: 0.0052, Val Loss: 0.0051\nValidation loss decreased (0.0158 --> 0.0051).  Saving model ...\nEpoch number14: Train Loss:0.005233656160829388, Val Loss:0.005121793400813207\nval accuracy: 0.9983193277310924\nEpoch 15/20, Train Loss: 0.0045, Val Loss: 0.0040\nValidation loss decreased (0.0051 --> 0.0040).  Saving model ...\nEpoch number15: Train Loss:0.004460621174050185, Val Loss:0.003992219344374961\nval accuracy: 0.9991596638655462\nEpoch 16/20, Train Loss: 0.0078, Val Loss: 0.0026\nValidation loss decreased (0.0040 --> 0.0026).  Saving model ...\nEpoch number16: Train Loss:0.007812300658548582, Val Loss:0.0025919244953865545\nval accuracy: 0.9946778711484594\nEpoch 17/20, Train Loss: 0.0073, Val Loss: 0.0181\nEpoch number17: Train Loss:0.007264600881258618, Val Loss:0.018148178693966246\nval accuracy: 0.9997198879551821\nEpoch 18/20, Train Loss: 0.0059, Val Loss: 0.0011\nValidation loss decreased (0.0026 --> 0.0011).  Saving model ...\nEpoch number18: Train Loss:0.005933670507290512, Val Loss:0.0011347663204194795\nval accuracy: 0.9991596638655462\nEpoch 19/20, Train Loss: 0.0062, Val Loss: 0.0019\nEpoch number19: Train Loss:0.006243580138431618, Val Loss:0.0018788686983322552\nval accuracy: 0.984313725490196\nEpoch 20/20, Train Loss: 0.0163, Val Loss: 0.0569\nEpoch number20: Train Loss:0.016306855114117134, Val Loss:0.056877467543387614\ndecoding feature: shape\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 11.1051, saved_times: 18\nEpoch [200/300], Loss: 11.4388, saved_times: 2\nEpoch [300/300], Loss: 4.7875, saved_times: 3\nBest model saved with loss: 3.8112\ndecoder train Accuracy for layer _conv_head: 0.95\ntest Accuracy for layer _conv_head: 0.49\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.2671, saved_times: 26\nEpoch [200/300], Loss: 1.1343, saved_times: 7\nEpoch [300/300], Loss: 0.9869, saved_times: 6\nBest model saved with loss: 0.9869\ndecoder train Accuracy for layer _avg_pooling: 0.66\ntest Accuracy for layer _avg_pooling: 0.33\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.9083, saved_times: 28\nEpoch [200/300], Loss: 1.8826, saved_times: 14\nEpoch [300/300], Loss: 1.8735, saved_times: 10\nBest model saved with loss: 1.8689\ndecoder train Accuracy for layer _fc: 0.24\ntest Accuracy for layer _fc: 0.21\ndecoding feature: color\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 0.0000, saved_times: 10\nEpoch [200/300], Loss: 0.0000, saved_times: 0\nEpoch [300/300], Loss: 0.0000, saved_times: 0\nBest model saved with loss: 0.0000\ndecoder train Accuracy for layer _conv_head: 1.00\ntest Accuracy for layer _conv_head: 0.98\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 0.0200, saved_times: 12\nEpoch [200/300], Loss: 0.0258, saved_times: 6\nEpoch [300/300], Loss: 0.0083, saved_times: 1\nBest model saved with loss: 0.0025\ndecoder train Accuracy for layer _avg_pooling: 1.00\ntest Accuracy for layer _avg_pooling: 0.98\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 0.0244, saved_times: 63\nEpoch [200/300], Loss: 0.0174, saved_times: 25\nEpoch [300/300], Loss: 0.0149, saved_times: 19\nBest model saved with loss: 0.0142\ndecoder train Accuracy for layer _fc: 1.00\ntest Accuracy for layer _fc: 0.98\ndecoding feature: texture\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 17.5951, saved_times: 19\nEpoch [200/300], Loss: 31.8206, saved_times: 5\nEpoch [300/300], Loss: 20.5801, saved_times: 5\nBest model saved with loss: 3.4644\ndecoder train Accuracy for layer _conv_head: 0.90\ntest Accuracy for layer _conv_head: 0.28\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.5935, saved_times: 20\nEpoch [200/300], Loss: 1.4393, saved_times: 6\nEpoch [300/300], Loss: 1.4019, saved_times: 5\nBest model saved with loss: 1.2955\ndecoder train Accuracy for layer _avg_pooling: 0.52\ntest Accuracy for layer _avg_pooling: 0.31\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.9279, saved_times: 11\nEpoch [200/300], Loss: 1.9239, saved_times: 2\nEpoch [300/300], Loss: 1.9242, saved_times: 3\nBest model saved with loss: 1.9162\ndecoder train Accuracy for layer _fc: 0.17\ntest Accuracy for layer _fc: 0.15\nnetwork model train target feature: texture\nDevice: cuda\ntraining network model for target feature: texture\nList of texture hold out classes: ['noise', 'hexgrid', 'rain']\n0-indexed dictionary of remaining items: {'solid': 0, 'stripes': 1, 'grid': 2, 'dots': 3, 'triangles': 4, 'zigzags': 5, 'pluses': 6}\nList of texture hold out classes: ['noise', 'hexgrid', 'rain']\n0-indexed dictionary of remaining items: {'solid': 0, 'stripes': 1, 'grid': 2, 'dots': 3, 'triangles': 4, 'zigzags': 5, 'pluses': 6}\ninf\nval accuracy: 0.14285714285714285\nEpoch 1/20, Train Loss: 1.5651, Val Loss: 1.9655\nValidation loss decreased (inf --> 1.9655).  Saving model ...\nEpoch number1: Train Loss:1.5651124593467824, Val Loss:1.9654530622712036\nval accuracy: 0.14285714285714285\nEpoch 2/20, Train Loss: 1.0484, Val Loss: 2.1967\nEpoch number2: Train Loss:1.0483647750000913, Val Loss:2.1967077432894238\nval accuracy: 0.14285714285714285\nEpoch 3/20, Train Loss: 0.6924, Val Loss: 2.6434\nEpoch number3: Train Loss:0.6923735083365927, Val Loss:2.6434402965364003\nval accuracy: 0.14285714285714285\nEpoch 4/20, Train Loss: 0.4280, Val Loss: 3.2921\nEpoch number4: Train Loss:0.42803259486707235, Val Loss:3.292101918415529\nval accuracy: 0.14285714285714285\nEpoch 5/20, Train Loss: 0.1841, Val Loss: 3.4947\nEpoch number5: Train Loss:0.1841368483124252, Val Loss:3.4947438782336664\nval accuracy: 0.14285714285714285\nEpoch 6/20, Train Loss: 0.1820, Val Loss: 4.2042\nEpoch number6: Train Loss:0.18195656259970483, Val Loss:4.2042392850923935\nval accuracy: 0.19607843137254902\nEpoch 7/20, Train Loss: 0.1019, Val Loss: 3.6022\nEpoch number7: Train Loss:0.10193207262860443, Val Loss:3.6021790456371146\nval accuracy: 0.3815126050420168\nEpoch 8/20, Train Loss: 0.1156, Val Loss: 2.4651\nEpoch number8: Train Loss:0.1155973119373398, Val Loss:2.465147677857001\nval accuracy: 0.6324929971988795\nEpoch 9/20, Train Loss: 0.0552, Val Loss: 1.0339\nValidation loss decreased (1.9655 --> 1.0339).  Saving model ...\nEpoch number9: Train Loss:0.05515553010274648, Val Loss:1.0339152701762544\nval accuracy: 0.742296918767507\nEpoch 10/20, Train Loss: 0.0384, Val Loss: 0.7768\nValidation loss decreased (1.0339 --> 0.7768).  Saving model ...\nEpoch number10: Train Loss:0.03837773762724794, Val Loss:0.776810953430101\nval accuracy: 0.8509803921568627\nEpoch 11/20, Train Loss: 0.0607, Val Loss: 0.4830\nValidation loss decreased (0.7768 --> 0.4830).  Saving model ...\nEpoch number11: Train Loss:0.06070590282059967, Val Loss:0.4829506715472673\nval accuracy: 0.8739495798319328\nEpoch 12/20, Train Loss: 0.0595, Val Loss: 0.3795\nValidation loss decreased (0.4830 --> 0.3795).  Saving model ...\nEpoch number12: Train Loss:0.059532944167320655, Val Loss:0.37952626836733994\nval accuracy: 0.796078431372549\nEpoch 13/20, Train Loss: 0.0280, Val Loss: 0.7666\nEpoch number13: Train Loss:0.028009151026291593, Val Loss:0.7666227957781624\nval accuracy: 0.9170868347338935\nEpoch 14/20, Train Loss: 0.0428, Val Loss: 0.4169\nEpoch number14: Train Loss:0.04278974650813261, Val Loss:0.4168851958269499\nval accuracy: 0.6778711484593838\nEpoch 15/20, Train Loss: 0.0328, Val Loss: 2.3366\nEpoch number15: Train Loss:0.03280692733715644, Val Loss:2.336559301931985\nval accuracy: 0.7686274509803922\nEpoch 16/20, Train Loss: 0.0160, Val Loss: 1.5502\nEpoch number16: Train Loss:0.01601090683729412, Val Loss:1.5502108234651282\nval accuracy: 0.9277310924369748\nEpoch 17/20, Train Loss: 0.0466, Val Loss: 0.2539\nValidation loss decreased (0.3795 --> 0.2539).  Saving model ...\nEpoch number17: Train Loss:0.04661233950319426, Val Loss:0.25387685056827985\nval accuracy: 0.8666666666666667\nEpoch 18/20, Train Loss: 0.0113, Val Loss: 0.5126\nEpoch number18: Train Loss:0.011265873173441814, Val Loss:0.51260602190381\nval accuracy: 0.9095238095238095\nEpoch 19/20, Train Loss: 0.0244, Val Loss: 0.2474\nValidation loss decreased (0.2539 --> 0.2474).  Saving model ...\nEpoch number19: Train Loss:0.02438879062532944, Val Loss:0.24742661650274314\nval accuracy: 0.9025210084033614\nEpoch 20/20, Train Loss: 0.0185, Val Loss: 0.2715\nEpoch number20: Train Loss:0.018535115135393753, Val Loss:0.27145972430539067\ndecoding feature: shape\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nList of shape hold out classes: ['triangle', 'plus', 'tee']\n0-indexed dictionary of remaining items: {'square': 0, 'circle': 1, 'rhombus': 2, 'pentagon': 3, 'star': 4, 'fivesquare': 5, 'trapezoid': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 37.3211, saved_times: 9\nEpoch [200/300], Loss: 39.1487, saved_times: 3\nEpoch [300/300], Loss: 34.6421, saved_times: 1\nBest model saved with loss: 27.2732\ndecoder train Accuracy for layer _conv_head: 0.62\ntest Accuracy for layer _conv_head: 0.39\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.7756, saved_times: 19\nEpoch [200/300], Loss: 1.5931, saved_times: 4\nEpoch [300/300], Loss: 1.5846, saved_times: 8\nBest model saved with loss: 1.5199\ndecoder train Accuracy for layer _avg_pooling: 0.41\ntest Accuracy for layer _avg_pooling: 0.28\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.8552, saved_times: 30\nEpoch [200/300], Loss: 1.8369, saved_times: 17\nEpoch [300/300], Loss: 1.8309, saved_times: 9\nBest model saved with loss: 1.8258\ndecoder train Accuracy for layer _fc: 0.25\ntest Accuracy for layer _fc: 0.20\ndecoding feature: color\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nList of color hold out classes: ['yellow', 'white', 'green']\n0-indexed dictionary of remaining items: {'red': 0, 'blue': 1, 'pink': 2, 'cyan': 3, 'purple': 4, 'ocean': 5, 'orange': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 48.9973, saved_times: 13\nEpoch [200/300], Loss: 54.0446, saved_times: 1\nEpoch [300/300], Loss: 48.7252, saved_times: 0\nBest model saved with loss: 42.6728\ndecoder train Accuracy for layer _conv_head: 0.29\ntest Accuracy for layer _conv_head: 0.21\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 1.6720, saved_times: 20\nEpoch [200/300], Loss: 1.5851, saved_times: 5\nEpoch [300/300], Loss: 1.5701, saved_times: 3\nBest model saved with loss: 1.5492\ndecoder train Accuracy for layer _avg_pooling: 0.40\ntest Accuracy for layer _avg_pooling: 0.27\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 1.7462, saved_times: 31\nEpoch [200/300], Loss: 1.7482, saved_times: 6\nEpoch [300/300], Loss: 1.7447, saved_times: 1\nBest model saved with loss: 1.7375\ndecoder train Accuracy for layer _fc: 0.29\ntest Accuracy for layer _fc: 0.22\ndecoding feature: texture\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nActivation of layer _conv_head: Shape=3430\nActivation of layer _avg_pooling: Shape=3430\nActivation of layer _fc: Shape=3430\nActivation of layer _conv_head: Shape=3570\nActivation of layer _avg_pooling: Shape=3570\nActivation of layer _fc: Shape=3570\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nList of texture hold out classes: ['rain', 'zigzags', 'stripes']\n0-indexed dictionary of remaining items: {'solid': 0, 'grid': 1, 'hexgrid': 2, 'dots': 3, 'noise': 4, 'triangles': 5, 'pluses': 6}\nlearning rate for _conv_head = 0.01\nEpoch [100/300], Loss: 10.2261, saved_times: 6\nEpoch [200/300], Loss: 8.1144, saved_times: 3\nEpoch [300/300], Loss: 9.0404, saved_times: 0\nBest model saved with loss: 5.1297\ndecoder train Accuracy for layer _conv_head: 0.92\ntest Accuracy for layer _conv_head: 0.91\nlearning rate for _avg_pooling = 0.01\nEpoch [100/300], Loss: 0.2146, saved_times: 13\nEpoch [200/300], Loss: 0.2093, saved_times: 3\nEpoch [300/300], Loss: 0.2103, saved_times: 1\nBest model saved with loss: 0.1964\ndecoder train Accuracy for layer _avg_pooling: 0.90\ntest Accuracy for layer _avg_pooling: 0.92\nlearning rate for _fc = 0.01\nEpoch [100/300], Loss: 0.2196, saved_times: 59\nEpoch [200/300], Loss: 0.2104, saved_times: 18\nEpoch [300/300], Loss: 0.2106, saved_times: 7\nBest model saved with loss: 0.2054\ndecoder train Accuracy for layer _fc: 0.90\ntest Accuracy for layer _fc: 0.91\ndecoding accuracies: {'shape': {'shape': {'_conv_head': (1.0, 0.9896358543417367), '_avg_pooling': (0.99533527696793, 0.9915966386554622), '_fc': (0.9711370262390671, 0.9865546218487395)}, 'color': {'_conv_head': (0.9935860058309038, 0.43669467787114846), '_avg_pooling': (0.7384839650145772, 0.3619047619047619), '_fc': (0.22157434402332363, 0.1711484593837535)}, 'texture': {'_conv_head': (0.9991253644314869, 0.22156862745098038), '_avg_pooling': (0.441399416909621, 0.2218487394957983), '_fc': (0.17900874635568514, 0.16022408963585436)}}, 'untrained': {'shape': {'_conv_head': (0.14285714285714285, 0.14285714285714285), '_avg_pooling': (0.14285714285714285, 0.14285714285714285), '_fc': (0.14285714285714285, 0.14285714285714285)}, 'color': {'_conv_head': (0.14285714285714285, 0.14285714285714285), '_avg_pooling': (0.14285714285714285, 0.14285714285714285), '_fc': (0.14285714285714285, 0.14285714285714285)}, 'texture': {'_conv_head': (0.14285714285714285, 0.14285714285714285), '_avg_pooling': (0.14285714285714285, 0.14285714285714285), '_fc': (0.14285714285714285, 0.14285714285714285)}}, 'color': {'shape': {'_conv_head': (0.9533527696793003, 0.48935574229691875), '_avg_pooling': (0.6618075801749271, 0.3302521008403361), '_fc': (0.23877551020408164, 0.2053221288515406)}, 'color': {'_conv_head': (1.0, 0.9778711484593837), '_avg_pooling': (0.9997084548104956, 0.9817927170868347), '_fc': (0.9956268221574344, 0.9795518207282913)}, 'texture': {'_conv_head': (0.902332361516035, 0.280672268907563), '_avg_pooling': (0.5201166180758018, 0.3100840336134454), '_fc': (0.16676384839650146, 0.15210084033613444)}}, 'texture': {'shape': {'_conv_head': (0.6189504373177842, 0.38711484593837536), '_avg_pooling': (0.4134110787172012, 0.28291316526610644), '_fc': (0.24897959183673468, 0.2042016806722689)}, 'color': {'_conv_head': (0.29416909620991255, 0.20840336134453782), '_avg_pooling': (0.39825072886297375, 0.27086834733893556), '_fc': (0.287463556851312, 0.21596638655462186)}, 'texture': {'_conv_head': (0.9192419825072886, 0.9095238095238095), '_avg_pooling': (0.9, 0.9165266106442577), '_fc': (0.8979591836734694, 0.9070028011204482)}}}\n","output_type":"stream"}]}]}